#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¥–åŠ±å‡½æ•°æ¨¡å— - åŒ…å«æ‰€æœ‰sampleçº§åˆ«å’Œbatchçº§åˆ«çš„å¥–åŠ±å‡½æ•°
"""
import torch
import numpy as np
from datetime import datetime
try:
    from .dynamic_reward_scaler import set_dynamic_reward_config, tracer, get_scaled_weight, update_moving_average
    _DYNAMIC_AVAILABLE = True
except Exception as e:
    print(f"âš ï¸ åŠ¨æ€å¥–åŠ±æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨é™æ€å¥–åŠ±: {e}")
    _DYNAMIC_AVAILABLE = False
# ç§»é™¤novelsumç›¸å…³å¯¼å…¥ï¼Œä½†ä¿ç•™å¿…è¦çš„å·¥å…·å‡½æ•°

# å¯¼å…¥batchå¤šæ ·æ€§å¥–åŠ±æ¨¡å—
try:
    from .batch_diversity_reward import reward_batch_diversity, initialize_batch_diversity_calculator
    print("âœ… Batchå¤šæ ·æ€§å¥–åŠ±æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ Batchå¤šæ ·æ€§å¥–åŠ±æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    reward_batch_diversity = None
    initialize_batch_diversity_calculator = None

# å…¨å±€å˜é‡ï¼ˆå°†ä»ä¸»æ¨¡å—å¯¼å…¥ï¼‰
training_data_global = []
current_batch_index = 0
batch_size_global = 8
reward_calculator = None
novelsum_calculator = None
attr_loader = None
compliance_calculator = None
generation_history = []
reward_logs = []
current_training_step = 0
reward_call_counter = 0
current_prompt_attributes = {}
training_visualizer = None  # è®­ç»ƒå¯è§†åŒ–å™¨

# Sampleçº§åˆ«å¥–åŠ±æƒé‡ï¼ˆé»˜è®¤é…ç½®ï¼‰
SAMPLE_REWARDS_CONFIG = {
    'sentiment_consistency_weight': 0.25,  # æƒ…æ„Ÿæ ‡ç­¾ä¸€è‡´æ€§
    'attribute_compliance_weight': 0.15,   # å±æ€§è¦æ±‚ç¬¦åˆåº¦
    'length_compliance_weight': 0.10,      # é•¿åº¦è¦æ±‚ç¬¦åˆåº¦
}

# Batchçº§åˆ«å¥–åŠ±æƒé‡ï¼ˆé»˜è®¤é…ç½®ï¼‰
BATCH_REWARDS_CONFIG = {
    'batch_diversity_weight': 0.30,  # åŸºäºå±€éƒ¨å¯†åº¦çš„batchå¤šæ ·æ€§å¥–åŠ±
}

# ä¼˜åŒ–åçš„é…ç½®ï¼ˆå°†è¢«åŠ¨æ€æ›´æ–°ï¼‰
CURRENT_SAMPLE_REWARDS_CONFIG = SAMPLE_REWARDS_CONFIG.copy()
CURRENT_BATCH_REWARDS_CONFIG = BATCH_REWARDS_CONFIG.copy()

# =============================================================================
# å·¥å…·å‡½æ•°ï¼ˆä»novelsumæ¨¡å—è¿ç§»çš„å¿…è¦å‡½æ•°ï¼‰
# =============================================================================

def extract_text_content_global(text):
    """æå–æ–‡æœ¬å†…å®¹çš„å…¨å±€å‡½æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
    if not text:
        return ""
    
    # ç®€å•çš„æ–‡æœ¬æ¸…ç†
    text = str(text).strip()
    
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    import re
    text = re.sub(r'\s+', ' ', text)
    
    return text

def separate_prompt_and_generation_global(completion, prompt=""):
    """åˆ†ç¦»æç¤ºè¯å’Œç”Ÿæˆå†…å®¹çš„å…¨å±€å‡½æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
    if not completion:
        return "", ""
    
    completion = str(completion).strip()
    prompt = str(prompt).strip()
    
    # å¦‚æœæä¾›äº†promptï¼Œå°è¯•ä»completionä¸­ç§»é™¤å®ƒ
    if prompt and completion.startswith(prompt):
        generation = completion[len(prompt):].strip()
        return prompt, generation
    
    # å¦‚æœæ²¡æœ‰promptæˆ–æ— æ³•åˆ†ç¦»ï¼Œè¿”å›æ•´ä¸ªcompletionä½œä¸ºgeneration
    return prompt, completion

def get_current_batch_attributes(step, batch_size):
    """æ ¹æ®å½“å‰è®­ç»ƒæ­¥æ•°è·å–å¯¹åº”æ‰¹æ¬¡çš„å±æ€§ä¿¡æ¯"""
    global training_data_global, current_batch_index
    
    if not training_data_global:
        print("âš ï¸ è®­ç»ƒæ•°æ®æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨é»˜è®¤å±æ€§")
        return {'target_sentiment': 'neutral', 'length': 200}
    
    # ä½¿ç”¨å¾ªç¯è®¿é—®ï¼Œé¿å…è¶…å‡ºæ•°æ®èŒƒå›´
    total_samples = len(training_data_global)
    
    # ä½¿ç”¨æ¨¡è¿ç®—ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
    start_idx = (step * batch_size) % total_samples
    
    # å¦‚æœæ˜¯å¾ªç¯è®¿é—®ï¼Œç»™å‡ºæç¤ºä½†ä¸æŠ¥é”™
    if step * batch_size >= total_samples:
        epoch_num = (step * batch_size) // total_samples + 1
        if step % 50 == 0:  # æ¯50æ­¥æç¤ºä¸€æ¬¡ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
            print(f"ğŸ”„ è®­ç»ƒè¿›å…¥ç¬¬{epoch_num}è½®ï¼Œæ­¥æ•°{step}ï¼Œä½¿ç”¨å¾ªç¯æ•°æ®è®¿é—®")
    
    # è·å–å½“å‰æ‰¹æ¬¡çš„ç¬¬ä¸€ä¸ªæ ·æœ¬çš„original_input
    try:
        sample = training_data_global[start_idx]
        if 'original_input' in sample:
            from scripts_amazon.attribute_handler import extract_attributes_from_input
            attributes = extract_attributes_from_input(sample['original_input'])
            print(f"ğŸ¯ Step {step}: ä»æ•°æ®ä¸­æå–å±æ€§ - {attributes.get('target_sentiment', 'unknown')}")
            return attributes
        else:
            print(f"âš ï¸ æ ·æœ¬{start_idx}ç¼ºå°‘original_inputå­—æ®µ")
            return {'target_sentiment': 'neutral', 'cuisine': 'american', 'length': 200}
    except Exception as e:
        print(f"âš ï¸ è·å–æ‰¹æ¬¡å±æ€§æ—¶å‡ºé”™: {e}")
        return {'target_sentiment': 'neutral', 'cuisine': 'american', 'length': 200}

def extract_attributes_from_current_prompts(prompts):
    """ä»å½“å‰çš„promptsä¸­ç›´æ¥æå–çœŸå®çš„ç›®æ ‡å±æ€§"""
    if not prompts or len(prompts) == 0:
        return {}
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªpromptä½œä¸ºä»£è¡¨ï¼ˆåŒä¸€batchçš„promptåº”è¯¥æœ‰ç›¸åŒçš„è¦æ±‚ï¼‰
    first_prompt = prompts[0]
    
    # ä»promptä¸­æå–å±æ€§
    from scripts_amazon.attribute_handler import extract_attributes_from_input
    
    # ç›´æ¥ä»promptæ–‡æœ¬ä¸­æå–å±æ€§
    attributes = extract_attributes_from_input(first_prompt)
    
    return attributes

def update_current_prompt_attributes_from_prompts(prompts):
    """ä»å½“å‰çš„promptsæ›´æ–°å±æ€§ï¼ˆè¿™æ˜¯æ­£ç¡®çš„æ–¹å¼ï¼‰"""
    global current_prompt_attributes
    
    if prompts:
        current_prompt_attributes = extract_attributes_from_current_prompts(prompts)
        print(f"ğŸ¯ ä»promptæå–å±æ€§: æƒ…æ„Ÿ={current_prompt_attributes.get('target_sentiment', 'æœªçŸ¥')}")
    else:
        print("âš ï¸ æ²¡æœ‰æä¾›promptsï¼Œæ— æ³•æå–å±æ€§")

def update_current_prompt_attributes(step=None, batch_size=None):
    """æ›´æ–°å½“å‰æ‰¹æ¬¡çš„æç¤ºå±æ€§ï¼ˆä»è®­ç»ƒæ•°æ®ä¸­æå–æˆ–éšæœºç”Ÿæˆï¼‰- å·²å¼ƒç”¨ï¼Œåº”ä½¿ç”¨ä»promptæå–çš„æ–¹å¼"""
    global current_prompt_attributes, current_batch_index, batch_size_global
    
    if step is not None and batch_size is not None:
        current_prompt_attributes = get_current_batch_attributes(step, batch_size)
        print(f"ğŸ”„ ä½¿ç”¨å†å²æ•°æ®æ›´æ–°æç¤ºå±æ€§ï¼ˆå¯èƒ½ä¸å‡†ç¡®ï¼‰: {current_prompt_attributes}")
    elif step is not None:
        current_prompt_attributes = get_current_batch_attributes(step, batch_size_global)
        print(f"ğŸ”„ ä½¿ç”¨é»˜è®¤batch_sizeæ›´æ–°æç¤ºå±æ€§")
    else:
        print("âš ï¸ æœªæä¾›stepå‚æ•°ï¼Œæ— æ³•æ›´æ–°æç¤ºå±æ€§")

def log_reward_details(step, reward_type, completions, rewards, **kwargs):
    global reward_logs, training_visualizer
    
    sample_details = []
    for i, (completion, reward) in enumerate(zip(completions[:3], rewards[:3])):
        sample_details.append({
            'completion_preview': completion[:100],
            'reward': float(reward),
            'completion_length': len(completion)
        })
    
    reward_log = {
        'step': step,
        'reward_type': reward_type,
        'mean_reward': float(torch.mean(rewards)),
        'std_reward': float(torch.std(rewards)),
        'min_reward': float(torch.min(rewards)),
        'max_reward': float(torch.max(rewards)),
        'num_samples': len(rewards),
        'sample_details': sample_details,
        'timestamp': datetime.now().isoformat()
    }
    
    reward_logs.append(reward_log)
    
    # ä¼ é€’ç»™è®­ç»ƒå¯è§†åŒ–å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if training_visualizer is not None and hasattr(training_visualizer, 'record_reward_data'):
        training_visualizer.record_reward_data(step, reward_type, rewards, completions)

# =============================================================================
# Sampleçº§åˆ«å¥–åŠ±å‡½æ•°
# =============================================================================

def reward_sentiment_consistency_batch(completions, **kwargs):
    """æƒ…æ„Ÿæ ‡ç­¾ä¸€è‡´æ€§å¥–åŠ±"""
    global compliance_calculator, current_training_step, current_prompt_attributes
    rewards = []
    print(f"ğŸ¯ Step {current_training_step} - è¯„ä¼°æƒ…æ„Ÿæ ‡ç­¾ä¸€è‡´æ€§å¥–åŠ± ({len(completions)}ä¸ªæ ·æœ¬)")
    
    # Debug: æŸ¥çœ‹kwargså†…å®¹
    if current_training_step <= 2:  # å‰3æ­¥æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        print(f"ğŸ” Debug kwargs keys: {list(kwargs.keys())}")
        if 'prompts' in kwargs:
            print(f"ğŸ” Prompts available: {len(kwargs['prompts'])}")
            
        # æ˜¾ç¤ºå®Œæ•´çš„promptå’Œcompletion
        print("=" * 80)
        print(f"ğŸ“ STEP {current_training_step} - å®Œæ•´çš„PROMPTå’ŒCOMPLETIONå¯¹æ¯”")
        print("=" * 80)
        
        for i, completion in enumerate(completions[:2]):  # åªæ˜¾ç¤ºå‰2ä¸ªæ ·æœ¬
            print(f"\nğŸ”¸ æ ·æœ¬ {i+1}:")
            print(f"ğŸ“¥ å®Œæ•´PROMPT:")
            if 'prompts' in kwargs and i < len(kwargs['prompts']):
                prompt = kwargs['prompts'][i]
                print(f"'{prompt}'")
            else:
                print("  [PROMPTæœªæ‰¾åˆ°]")
            
            print(f"\nğŸ“¤ å®Œæ•´COMPLETION:")
            print(f"'{completion}'")
            
            print(f"\nğŸ” åˆ†ç¦»åçš„ç”Ÿæˆå†…å®¹:")
            try:
                if 'prompts' in kwargs and i < len(kwargs['prompts']):
                    prompt = kwargs['prompts'][i]
                    _, separated_generation = separate_prompt_and_generation_global(completion, prompt)
                else:
                    _, separated_generation = separate_prompt_and_generation_global(completion, "")
                print(f"'{separated_generation[:300]}...'")
            except Exception as e:
                print(f"   âŒ åˆ†ç¦»å¤±è´¥: {e}")
                separated_generation = completion
            
            print("-" * 60)
        print("=" * 80)
        
    # ä»promptä¸­æå–çœŸå®çš„ç›®æ ‡å±æ€§ï¼ˆæ­£ç¡®æ–¹å¼ï¼‰
    if 'prompts' in kwargs:
        update_current_prompt_attributes_from_prompts(kwargs['prompts'])
    else:
        # å¦‚æœæ²¡æœ‰promptsï¼Œå°è¯•æ—§æ–¹å¼ï¼ˆä½†ä¸å¤ªå‡†ç¡®ï¼‰
        if not current_prompt_attributes:
            update_current_prompt_attributes()
    
    target_sentiment = current_prompt_attributes.get('target_sentiment', 'neutral')
    
    for i, completion in enumerate(completions):
        # æ£€æŸ¥compliance_calculatoræ˜¯å¦ä¸ºNoneï¼ˆAmazonæ•°æ®é›†æƒ…å†µï¼‰
        if compliance_calculator is None:
            # ä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…ä½œä¸ºå…œåº•
            completion_lower = completion.lower()
            if target_sentiment.lower() in completion_lower:
                score = 1.0
            else:
                # æ£€æŸ¥æƒ…æ„Ÿç›¸å…³è¯æ±‡
                sentiment_words = {
                    'very positive': ['excellent', 'amazing', 'fantastic', 'outstanding', 'perfect', 'love', 'best'],
                    'positive': ['good', 'great', 'nice', 'satisfied', 'happy', 'recommend', 'works well'],
                    'neutral': ['okay', 'average', 'fine', 'decent', 'acceptable'],
                    'negative': ['bad', 'poor', 'disappointed', 'issues', 'problems', 'not good'],
                    'very negative': ['terrible', 'awful', 'horrible', 'worst', 'hate', 'waste', 'broken']
                }
                
                words = sentiment_words.get(target_sentiment.lower(), [])
                matches = sum(1 for word in words if word in completion_lower)
                score = min(matches / max(len(words), 1), 1.0)
        else:
            score = compliance_calculator.calculate_sentiment_consistency(completion, target_sentiment)
        
        # æ­£å‘æ¿€åŠ±ç­–ç•¥ï¼šå®Œå…¨åŒ¹é…ç»™é«˜å¥–åŠ±ï¼Œä¸åŒ¹é…ç»™ä¸­æ€§å¥–åŠ±ï¼ˆé¿å…è´Ÿåˆ†ï¼‰
        raw_norm = max(0.0, min(1.0, score))
        dyn_w = get_scaled_weight("sentiment", current_training_step) if _DYNAMIC_AVAILABLE else 0.5
        continuous_reward = (raw_norm ** 2) * dyn_w
        if score >= 1.0:
            legacy_reward = 0.5
        elif score >= 0.5:
            legacy_reward = 0.2
        else:
            legacy_reward = 0.0
        final_reward = max(legacy_reward, continuous_reward)
        ma = update_moving_average("sentiment_raw", raw_norm) if _DYNAMIC_AVAILABLE else raw_norm
        rewards.append(final_reward)
        
        if i < 2:
            if compliance_calculator is not None:
                extracted_sentiment = compliance_calculator.extract_sentiment_from_json(completion)
            else:
                # Amazonæ•°æ®é›†ï¼šä»completionä¸­æå–æƒ…æ„Ÿæ ‡ç­¾
                try:
                    import json
                    completion_json = json.loads(completion)
                    extracted_sentiment = completion_json.get('output', 'unknown')
                except (json.JSONDecodeError, KeyError, TypeError):
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…
                    completion_lower = completion.lower()
                    if any(word in completion_lower for word in ['excellent', 'amazing', 'fantastic', 'outstanding', 'perfect', 'love', 'best']):
                        extracted_sentiment = 'very positive'
                    elif any(word in completion_lower for word in ['good', 'great', 'nice', 'satisfied', 'happy', 'recommend']):
                        extracted_sentiment = 'positive'
                    elif any(word in completion_lower for word in ['okay', 'average', 'fine', 'decent', 'acceptable']):
                        extracted_sentiment = 'neutral'
                    elif any(word in completion_lower for word in ['bad', 'poor', 'disappointed', 'issues', 'problems']):
                        extracted_sentiment = 'negative'
                    elif any(word in completion_lower for word in ['terrible', 'awful', 'horrible', 'worst', 'hate', 'waste', 'broken']):
                        extracted_sentiment = 'very negative'
                    else:
                        extracted_sentiment = 'unknown'
            # æ˜¾ç¤ºå¤„ç†åçš„ç”Ÿæˆå†…å®¹è€Œä¸æ˜¯åŸå§‹completion
            generated_text = extract_text_content_global(completion)
            print(f"   æ ·æœ¬{i+1}: ç›®æ ‡æƒ…æ„Ÿ={target_sentiment}, æå–æƒ…æ„Ÿ={extracted_sentiment}, raw={raw_norm:.2f}, è¿ç»­={continuous_reward:.4f}, æœ€ç»ˆ={final_reward:.4f}, MA={ma:.3f}")
        # tracer åœ¨ dynamic_reward_scaler ä¸­å¯èƒ½æ˜¯ä¸€ä¸ªå±æ€§å¯¹è±¡è€Œéå¯è°ƒç”¨å‡½æ•°
        if _DYNAMIC_AVAILABLE and tracer is not None and hasattr(tracer, "log"):
            tracer.log(current_training_step, task="amazon", component="sentiment", raw_score=raw_norm, final_reward=final_reward,
                         extra={"legacy_reward": legacy_reward, "dyn_weight": dyn_w})
            print(f"   ç”Ÿæˆå†…å®¹: {generated_text[:100]}...")
    
    rewards_tensor = torch.tensor(rewards, dtype=torch.float32)
    
    # ä»kwargsä¸­ç§»é™¤stepå‚æ•°ï¼Œé¿å…é‡å¤ä¼ é€’
    kwargs_copy = kwargs.copy()
    kwargs_copy.pop('step', None)
    
    log_reward_details(current_training_step, "sentiment_consistency", completions, rewards_tensor, **kwargs_copy)
    return rewards_tensor

def reward_attribute_compliance_batch(completions, **kwargs):
    """å±æ€§è¦æ±‚ç¬¦åˆåº¦å¥–åŠ±"""
    global compliance_calculator, current_training_step, current_prompt_attributes
    rewards = []
    print(f"ğŸ” Step {current_training_step} - è¯„ä¼°å±æ€§ç¬¦åˆåº¦å¥–åŠ± ({len(completions)}ä¸ªæ ·æœ¬)")
    
    # ç¡®ä¿æœ‰å½“å‰æç¤ºå±æ€§
    if not current_prompt_attributes:
        update_current_prompt_attributes()
    
    for i, completion in enumerate(completions):
        total_score = 0.0
        attribute_count = 0
        
        # è¯„ä¼°èœç³»å±æ€§
        if 'cuisine' in current_prompt_attributes:
            cuisine_score = compliance_calculator.calculate_cuisine_compliance(
                completion, current_prompt_attributes['cuisine']
            )
            total_score += cuisine_score
            attribute_count += 1
        
        # è¯„ä¼°å…¶ä»–å±æ€§
        for attr_name in ['style', 'price_range', 'service_quality', 'atmosphere']:
            if attr_name in current_prompt_attributes:
                attr_score = compliance_calculator.calculate_attribute_keyword_match(
                    completion, 
                    attr_name, 
                    current_prompt_attributes[attr_name],
                    current_prompt_attributes.get('target_sentiment')
                )
                total_score += attr_score * 0.5  # é™ä½å…¶ä»–å±æ€§çš„æƒé‡
                attribute_count += 0.5
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        if attribute_count > 0:
            avg_score = total_score / attribute_count
        else:
            avg_score = 0.5
        
        # æ­£å‘æ¿€åŠ±ç­–ç•¥ï¼šç¬¦åˆå±æ€§ç»™å¥–åŠ±ï¼Œä¸ç¬¦åˆä¸æƒ©ç½š
        raw_norm = max(0.0, min(1.0, avg_score))
        dyn_w = get_scaled_weight("attribute", current_training_step) if _DYNAMIC_AVAILABLE else 0.25
        continuous_reward = (raw_norm ** 1.5) * dyn_w
        if avg_score >= 0.8:
            legacy_reward = 0.25
        elif avg_score >= 0.6:
            legacy_reward = 0.15
        elif avg_score >= 0.4:
            legacy_reward = 0.1
        elif avg_score >= 0.2:
            legacy_reward = 0.05
        else:
            legacy_reward = 0.0
        final_reward = max(legacy_reward, continuous_reward)
        ma = update_moving_average("attribute_raw", raw_norm) if _DYNAMIC_AVAILABLE else raw_norm
        rewards.append(final_reward)
        
        if i < 2:
            cuisine = current_prompt_attributes.get('cuisine', 'N/A')
            print(f"   æ ·æœ¬{i+1}: ç›®æ ‡èœç³»={cuisine}, raw={raw_norm:.2f}, è¿ç»­={continuous_reward:.4f}, æœ€ç»ˆ={final_reward:.4f}, MA={ma:.3f}")
        if _DYNAMIC_AVAILABLE and tracer is not None and hasattr(tracer, "log"):
            tracer.log(current_training_step, task="amazon", component="attribute", raw_score=raw_norm, final_reward=final_reward,
                         extra={"legacy_reward": legacy_reward, "dyn_weight": dyn_w})
    
    rewards_tensor = torch.tensor(rewards, dtype=torch.float32)
    
    # ä»kwargsä¸­ç§»é™¤stepå‚æ•°ï¼Œé¿å…é‡å¤ä¼ é€’
    kwargs_copy = kwargs.copy()
    kwargs_copy.pop('step', None)
    
    log_reward_details(current_training_step, "attribute_compliance", completions, rewards_tensor, **kwargs_copy)
    return rewards_tensor

def reward_length_compliance_batch(completions, **kwargs):
    """é•¿åº¦è¦æ±‚ç¬¦åˆåº¦å¥–åŠ±"""
    global compliance_calculator, current_training_step, current_prompt_attributes
    rewards = []
    print(f"ğŸ“ Step {current_training_step} - è¯„ä¼°é•¿åº¦ç¬¦åˆåº¦å¥–åŠ± ({len(completions)}ä¸ªæ ·æœ¬)")
    
    # ç¡®ä¿æœ‰å½“å‰æç¤ºå±æ€§
    if not current_prompt_attributes:
        update_current_prompt_attributes()
    
    target_length = current_prompt_attributes.get('length', 200)
    
    for i, completion in enumerate(completions):
        score = compliance_calculator.calculate_length_compliance(completion, target_length, tolerance=25)
        
        # æ­£å‘æ¿€åŠ±ç­–ç•¥ï¼šåœ¨èŒƒå›´å†…ç»™å¥–åŠ±ï¼Œè¶…å‡ºèŒƒå›´ä¸æƒ©ç½š
        raw_norm = max(0.0, min(1.0, score))
        dyn_w = get_scaled_weight("length", current_training_step) if _DYNAMIC_AVAILABLE else 0.2
        continuous_reward = (1 / (1 + np.exp(-8 * (raw_norm - 0.5)))) * dyn_w
        if score >= 1.0:
            legacy_reward = 0.2
        elif score >= 0.8:
            legacy_reward = 0.1
        elif score >= 0.5:
            legacy_reward = 0.05
        else:
            legacy_reward = 0.0
        final_reward = max(legacy_reward, continuous_reward)
        ma = update_moving_average("length_raw", raw_norm) if _DYNAMIC_AVAILABLE else raw_norm
        rewards.append(final_reward)
        
        if i < 2:
            # è®¡ç®—å®é™…é•¿åº¦ç”¨äºæ˜¾ç¤º
            text_content = extract_text_content_global(completion)
            actual_length = len(text_content.split())
            
            # æ˜¾ç¤ºç›®æ ‡é•¿åº¦ä¿¡æ¯
            if isinstance(target_length, dict):
                length_info = f"ç›®æ ‡èŒƒå›´={target_length['min']}-{target_length['max']}"
            else:
                length_info = f"ç›®æ ‡é•¿åº¦={target_length}"
            
            print(f"   æ ·æœ¬{i+1}: {length_info}, raw={raw_norm:.2f}, è¿ç»­={continuous_reward:.4f}, æœ€ç»ˆ={final_reward:.4f}, MA={ma:.3f}")
        if _DYNAMIC_AVAILABLE and tracer is not None and hasattr(tracer, "log"):
            tracer.log(current_training_step, task="amazon", component="length", raw_score=raw_norm, final_reward=final_reward,
                         extra={"legacy_reward": legacy_reward, "dyn_weight": dyn_w})
    
    rewards_tensor = torch.tensor(rewards, dtype=torch.float32)
    
    # ä»kwargsä¸­ç§»é™¤stepå‚æ•°ï¼Œé¿å…é‡å¤ä¼ é€’
    kwargs_copy = kwargs.copy()
    kwargs_copy.pop('step', None)
    
    log_reward_details(current_training_step, "length_compliance", completions, rewards_tensor, **kwargs_copy)
    return rewards_tensor

# =============================================================================
# ç”Ÿæˆè´¨é‡å¥–åŠ±å‡½æ•°
# =============================================================================

def extract_clean_review_global(generated_text, prompt=""):
    """ä»ç”Ÿæˆæ–‡æœ¬ä¸­æå–å¹²å‡€çš„è¯„è®ºå†…å®¹ï¼ˆä¿æŒJSONæ ¼å¼ï¼‰"""
    import re
    import json
    
    # ç§»é™¤promptéƒ¨åˆ†
    if prompt and generated_text.startswith(prompt):
        content = generated_text[len(prompt):].strip()
    else:
        content = generated_text.strip()
    
    # å°è¯•æå–å®Œæ•´çš„JSON
    json_pattern = r'\{[^{}]*"input"\s*:\s*"[^"]*"[^{}]*"output"\s*:\s*"[^"]*"[^{}]*\}'
    json_match = re.search(json_pattern, content)
    if json_match:
        try:
            # éªŒè¯JSONæ ¼å¼æ˜¯å¦æ­£ç¡®
            json_str = json_match.group(0)
            parsed_json = json.loads(json_str)
            if 'input' in parsed_json and 'output' in parsed_json:
                return json_str  # è¿”å›å®Œæ•´çš„JSON
        except json.JSONDecodeError:
            pass
    
    # å¦‚æœæ²¡æ‰¾åˆ°å®Œæ•´JSONï¼Œå°è¯•æå–inputå­—æ®µçš„å†…å®¹
    input_match = re.search(r'"input"\s*:\s*"([^"]*)"', content)
    if input_match:
        input_content = input_match.group(1)
        # å°è¯•æå–outputå­—æ®µ
        output_match = re.search(r'"output"\s*:\s*"([^"]*)"', content)
        if output_match:
            output_content = output_match.group(1)
            # é‡æ„JSON
            return f'{{"input": "{input_content}", "output": "{output_content}"}}'
        else:
            # åªæœ‰inputï¼Œæ·»åŠ é»˜è®¤output
            return f'{{"input": "{input_content}", "output": "unknown"}}'
    
    # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•æå–å¼•å·å†…çš„é•¿å†…å®¹
    quote_matches = re.findall(r'"([^"]{20,})"', content)
    if quote_matches:
        # é€‰æ‹©æœ€é•¿çš„åŒ¹é…ä½œä¸ºä¸»è¦å†…å®¹ï¼ŒåŒ…è£…æˆJSON
        longest_content = max(quote_matches, key=len)
        return f'{{"input": "Text: {longest_content}", "output": "unknown"}}'
    
    # æœ€åçš„å…œåº•ï¼šç§»é™¤æŒ‡ä»¤æ€§æ–‡æœ¬ï¼ŒåŒ…è£…æˆJSON
    clean_patterns = [
        r'Here is an example.*?:',
        r'Here\'s an example.*?:',
        r'Do NOT.*?\.',
        r'Note that.*?\.',
        r'```.*?```',
        r'""".*?"""',
        r'Example.*?:',
    ]
    
    for pattern in clean_patterns:
        content = re.sub(pattern, '', content, flags=re.DOTALL | re.IGNORECASE)
    
    # æ¸…ç†å¤šä½™ç©ºç™½å’Œç‰¹æ®Šå­—ç¬¦
    content = re.sub(r'\s+', ' ', content).strip()
    content = re.sub(r'^["\'\s]+|["\'\s]+$', '', content)
    
    if len(content) > 10:
        return f'{{"input": "Text: {content}", "output": "unknown"}}'
    else:
        return '{"input": "Text: Invalid generation", "output": "unknown"}'

def calculate_generation_quality_score_global(generated_text, prompt=""):
    """ç”Ÿæˆè´¨é‡è¯„åˆ†ï¼ˆå‡è½»è¿‡åº¦æ‰£åˆ†ï¼Œä¿æŒJSONç»“æ„ä¸»è·¯å¾„åŠ åˆ†ï¼‰"""
    import json
    import re

    clean_content = extract_clean_review_global(generated_text, prompt)

    # æé«˜èµ·å§‹åŸºçº¿ï¼Œé˜²æ­¢ä¸€æ¬¡æ‰£åˆ†ç›´æ¥å½’é›¶
    quality_score = 0.2

    # ç¼–ç¨‹/æ¨¡æ¿ç±»ä¸è‰¯æ¨¡å¼ï¼ˆè½»é‡åŒ–æƒ©ç½šï¼‰
    bad_patterns = [
        'import ', 'def ', 'class ', 'function', '```', 'python', 'code', 'script',
        'return ', 'print(', 'if __name__', 'from ', 'pipeline', 'random.',
        'tokenizer', 'transformers', 'torch', 'numpy'
    ]
    bad_count = sum(1 for pattern in bad_patterns if pattern.lower() in generated_text.lower())
    if bad_count > 0:
        quality_score -= min(0.6, bad_count * 0.2)  # ä¸Šé™é™ä½

    # é‡å¤å­—ç¬¦æƒ©ç½šå‡è½»
    repeat_matches = re.findall(r'(.)\1{10,}', generated_text)
    if repeat_matches:
        quality_score -= 0.3

    # æŒ‡ä»¤æ€§çŸ­è¯­æ‰£åˆ†å‡è½»
    instruction_phrases = [
        'here is an example', 'here\'s an example', 'do not add', 'note that you',
        'template', 'format', 'please see', 'answer:', 'step 1:', 'step 2:',
        'feel free', 'let me know', 'best regards'
    ]
    bad_instructions = sum(1 for phrase in instruction_phrases if phrase.lower() in generated_text.lower())
    if bad_instructions > 0:
        quality_score -= min(0.3, bad_instructions * 0.1)

    # å¥–åŠ±ç¨³å®š JSON ç»“æ„
    try:
        parsed_json = json.loads(clean_content)
        if 'input' in parsed_json and 'output' in parsed_json:
            quality_score += 0.4
            input_content = parsed_json.get('input', '')
            if input_content.startswith('Text: ') and len(input_content) > 20:
                quality_score += 0.3
                review_text = input_content[6:]
                word_count = len(review_text.split())
                # é•¿åº¦å¥–åŠ±æ”¹ä¸ºæ›´å®½å®¹ï¼šè¿‡çŸ­ä»æ‰£åˆ†ï¼Œè¿‡é•¿è½»å¾®æ‰£åˆ†
                if word_count < 10:
                    quality_score -= 0.2
                elif word_count > 500:
                    quality_score -= 0.05
                else:
                    # å¹³æ»‘é•¿åº¦å¥–åŠ±ï¼š20~300 æ¥è¿‘æ»¡åˆ†ï¼Œå…¶ä½™é€æ¸ä¸‹é™
                    if word_count < 20:
                        length_bonus = (word_count / 20) * 0.2
                    elif word_count <= 300:
                        length_bonus = 0.2
                    else:
                        # 300~500 çº¿æ€§è¡°å‡åˆ° 0.05
                        length_bonus = max(0.05, 0.2 - (word_count - 300) / 200 * 0.15)
                    quality_score += length_bonus
            else:
                quality_score -= 0.05  # å‡è½» input æ ¼å¼æƒ©ç½š

            output_content = parsed_json.get('output', '')
            valid_sentiments = ['very negative', 'negative', 'neutral', 'positive', 'very positive']
            if output_content in valid_sentiments:
                quality_score += 0.1
            elif output_content == 'unknown':
                pass  # ä¸å†æ‰£åˆ†
            else:
                quality_score -= 0.1
        else:
            quality_score -= 0.15  # ç¼ºå°‘å­—æ®µæƒ©ç½šå‡è½»
    except json.JSONDecodeError:
        quality_score -= 0.25  # JSONå¤±è´¥æƒ©ç½šå‡è½»

    # å™ªéŸ³/æ ¼å¼åŒ–ç¬¦å·æƒ©ç½šå‡è½»
    noise_patterns = [r'```', r'"""', r'\{[^}]*\}[^}]*\{', r'\\n\\n\\n+']
    noise_hits = 0
    for pattern in noise_patterns:
        if re.search(pattern, generated_text):
            noise_hits += 1
    if noise_hits:
        quality_score -= min(0.15, noise_hits * 0.05)

    # ç§»é™¤é‡å¤ code_indicators äºŒæ¬¡æƒ©ç½šï¼ˆå·²åœ¨ bad_patterns è¦†ç›–ï¼‰

    return max(0.0, min(1.0, quality_score))

def reward_generation_quality_batch(completions, **kwargs):
    """ç”Ÿæˆè´¨é‡å¥–åŠ±å‡½æ•°"""
    global current_training_step
    rewards = []
    prompts = kwargs.get('prompts', [''] * len(completions))
    
    print(f"ğŸ¨ Step {current_training_step} - è¯„ä¼°ç”Ÿæˆè´¨é‡å¥–åŠ± ({len(completions)}ä¸ªæ ·æœ¬)")
    
    for i, completion in enumerate(completions):
        prompt = prompts[i] if i < len(prompts) else ""
        quality_score = calculate_generation_quality_score_global(completion, prompt)
        
        # æ­£å‘æ¿€åŠ±ç­–ç•¥ï¼šé«˜è´¨é‡ç»™å¥–åŠ±ï¼Œä½è´¨é‡ä¸æƒ©ç½š
        raw_norm = max(0.0, min(1.0, quality_score))
        dyn_w = get_scaled_weight("generation_quality", current_training_step) if _DYNAMIC_AVAILABLE else 0.3
        continuous_reward = (raw_norm ** 1.2) * dyn_w
        if quality_score >= 0.9:
            legacy_reward = 0.3
        elif quality_score >= 0.7:
            legacy_reward = 0.2
        elif quality_score >= 0.5:
            legacy_reward = 0.1
        elif quality_score >= 0.3:
            legacy_reward = 0.05
        else:
            legacy_reward = 0.0
        reward = max(legacy_reward, continuous_reward)
        ma = update_moving_average("generation_quality_raw", raw_norm) if _DYNAMIC_AVAILABLE else raw_norm
        rewards.append(reward)
        
        if i < 2:  # æ˜¾ç¤ºå‰2ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
            clean_content = extract_clean_review_global(completion, prompt)
            print(f"   æ ·æœ¬{i+1}: rawè´¨é‡={raw_norm:.2f}, è¿ç»­={continuous_reward:.4f}, æœ€ç»ˆ={reward:.4f}, MA={ma:.3f}")
        if _DYNAMIC_AVAILABLE and tracer is not None and hasattr(tracer, "log"):
            tracer.log(current_training_step, task="amazon", component="generation_quality", raw_score=raw_norm, final_reward=reward,
                         extra={"legacy_reward": legacy_reward, "dyn_weight": dyn_w})
            print(f"   æ¸…ç†åå†…å®¹: {clean_content[:100]}...")
    
    rewards_tensor = torch.tensor(rewards, dtype=torch.float32)
    
    # ä»kwargsä¸­ç§»é™¤stepå‚æ•°ï¼Œé¿å…é‡å¤ä¼ é€’
    kwargs_copy = kwargs.copy()
    kwargs_copy.pop('step', None)
    
    log_reward_details(current_training_step, "generation_quality", completions, rewards_tensor, **kwargs_copy)
    return rewards_tensor

# =============================================================================
# Batchçº§åˆ«å¥–åŠ±å‡½æ•°
# =============================================================================

def reward_yelp_semantic_diversity_batch(completions, **kwargs):
    """é¤å…è¯„è®ºè¯­ä¹‰å¤šæ ·æ€§å¥–åŠ±ï¼ˆç»“åˆNovelSumæ€æƒ³å’ŒYelpæ•°æ®ç‰¹è‰²ï¼Œæ”¯æŒåŠ¨æ€å‚è€ƒæ•°æ®ï¼‰"""
    global novelsum_calculator, current_training_step
    
    print(f"ğŸ½ï¸ Step {current_training_step} - è¯„ä¼°é¤å…è¯„è®ºè¯­ä¹‰å¤šæ ·æ€§å¥–åŠ± (batch size: {len(completions)})")
    
    try:
        # ä»completionsä¸­æå–å®é™…æ–‡æœ¬å†…å®¹
        texts = [extract_text_content_global(completion) for completion in completions]
        texts = [text for text in texts if text and len(text) > 10]
        
        if len(texts) < 2:
            print("   âš ï¸ æœ‰æ•ˆæ–‡æœ¬å°‘äº2ä¸ªï¼Œè¿”å›åŸºç¡€å¥–åŠ±")
            return torch.zeros(len(completions), dtype=torch.float32)
        
        # ä½¿ç”¨NovelSumè®¡ç®—å¤šæ ·æ€§
        novelsum_score = novelsum_calculator.calculate_novelsum_score(
            texts,
            density_power=0.5,
            distance_power=1.0,
            neighbors=min(10, len(texts))
        )
        
        # è®¡ç®—é¤å…è¯„è®ºç‰¹è‰²å¤šæ ·æ€§
        print(f"ğŸ” DEBUG: å¼€å§‹è®¡ç®—é¤å…å¤šæ ·æ€§...")
        restaurant_diversity = calculate_restaurant_specific_diversity(texts)
        print(f"ğŸ” DEBUG: é¤å…å¤šæ ·æ€§è®¡ç®—å®Œæˆ: {restaurant_diversity} (ç±»å‹: {type(restaurant_diversity)})")
        
        # ç»“åˆä¸¤ç§å¤šæ ·æ€§åº¦é‡
        print(f"ğŸ” DEBUG: å¼€å§‹ç»“åˆå¤šæ ·æ€§åº¦é‡...")
        print(f"ğŸ” DEBUG: novelsum_score = {novelsum_score} (ç±»å‹: {type(novelsum_score)})")
        print(f"ğŸ” DEBUG: restaurant_diversity = {restaurant_diversity} (ç±»å‹: {type(restaurant_diversity)})")
        
        combined_diversity = (novelsum_score * 0.6 + restaurant_diversity * 0.4)
        print(f"ğŸ” DEBUG: åˆå§‹combined_diversity = {combined_diversity} (ç±»å‹: {type(combined_diversity)})")
        
        combined_diversity = float(combined_diversity)  # ç¡®ä¿æ˜¯æ ‡é‡
        print(f"ğŸ” DEBUG: è½¬æ¢åcombined_diversity = {combined_diversity} (ç±»å‹: {type(combined_diversity)})")
        
        # å°†batchçº§åˆ«çš„å¥–åŠ±åˆ†é…ç»™æ¯ä¸ªæ ·æœ¬
        print(f"ğŸ” DEBUG: å¼€å§‹è®¡ç®—batchå¥–åŠ±...")
        batch_reward = (combined_diversity - 0.5) * 0.6  # æ ‡å‡†åŒ–åˆ°[-0.3, 0.3]
        print(f"ğŸ” DEBUG: åˆå§‹batch_reward = {batch_reward} (ç±»å‹: {type(batch_reward)})")
        
        batch_reward = float(batch_reward)  # ç¡®ä¿æ˜¯æ ‡é‡
        print(f"ğŸ” DEBUG: è½¬æ¢åbatch_reward = {batch_reward} (ç±»å‹: {type(batch_reward)})")
        
        rewards = torch.full((len(completions),), batch_reward, dtype=torch.float32)
        print(f"ğŸ” DEBUG: rewardså¼ é‡åˆ›å»ºå®Œæˆ: shape={rewards.shape}")
        
        # åŠ¨æ€æ›´æ–°å‚è€ƒæ•°æ®æ± ï¼ˆæ›´å®½æ¾çš„æ¡ä»¶ä»¥ä¿ƒè¿›å­¦ä¹ ï¼‰
        print(f"ğŸ” DEBUG: æ£€æŸ¥æ˜¯å¦æ›´æ–°å‚è€ƒæ± ...")
        print(f"ğŸ” DEBUG: hasattr(novelsum_calculator, 'add_training_samples') = {hasattr(novelsum_calculator, 'add_training_samples')}")
        print(f"ğŸ” DEBUG: batch_reward = {batch_reward}, batch_reward > -0.1 = {batch_reward > -0.1}")
        
        if hasattr(novelsum_calculator, 'add_training_samples') and batch_reward > -0.1:
            try:
                # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„è´¨é‡åˆ†æ•°ï¼ˆåŸºäºå¤šæ ·æ€§å’Œé•¿åº¦ï¼‰
                quality_scores = []
                for text in texts:
                    length_score = min(1.0, len(text.split()) / 150.0)  # é•¿åº¦å½’ä¸€åŒ–
                    
                    # å®‰å…¨åœ°è®¡ç®—individual_novelty
                    if hasattr(novelsum_calculator, 'reference_manager'):
                        individual_novelty = novelsum_calculator.reference_manager.compute_novelty_score(text)
                        # ç¡®ä¿æ˜¯æ ‡é‡
                        if isinstance(individual_novelty, torch.Tensor):
                            individual_novelty = float(individual_novelty.item())
                        else:
                            individual_novelty = float(individual_novelty)
                    else:
                        individual_novelty = 0.5
                    
                    quality = (individual_novelty * 0.7 + length_score * 0.3)
                    quality_scores.append(float(quality))
                
                # æ·»åŠ é«˜è´¨é‡æ ·æœ¬åˆ°å‚è€ƒæ± 
                added_count = novelsum_calculator.add_training_samples(texts, quality_scores)
                if added_count > 0:
                    print(f"   ğŸ“Š æ·»åŠ {added_count}ä¸ªé«˜è´¨é‡æ ·æœ¬åˆ°åŠ¨æ€å‚è€ƒæ± ")
                    
            except Exception as e:
                print(f"   âš ï¸ åŠ¨æ€å‚è€ƒæ± æ›´æ–°å¤±è´¥: {e}")
                import traceback
                print(f"   è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        
        print(f"   NovelSumåˆ†æ•°: {novelsum_score:.3f}, é¤å…å¤šæ ·æ€§: {restaurant_diversity:.3f}")
        print(f"   ç»¼åˆå¤šæ ·æ€§: {combined_diversity:.3f}, batchå¥–åŠ±: {batch_reward:.4f}")
        
        # å®šæœŸæ‰“å°å‚è€ƒæ± ç»Ÿè®¡ä¿¡æ¯
        if current_training_step % 20 == 0 and hasattr(novelsum_calculator, 'get_reference_statistics'):
            stats = novelsum_calculator.get_reference_statistics()
            print(f"   ğŸ“ˆ å‚è€ƒæ± çŠ¶æ€: {stats.get('total_count', 0)}æ ·æœ¬ (åŸå§‹:{stats.get('original_count', 0)}, åŠ¨æ€:{stats.get('dynamic_count', 0)})")
        
        # ä»kwargsä¸­ç§»é™¤stepå‚æ•°ï¼Œé¿å…é‡å¤ä¼ é€’
        kwargs_copy = kwargs.copy()
        kwargs_copy.pop('step', None)
        log_reward_details(current_training_step, "yelp_semantic_diversity", completions, rewards, **kwargs_copy)
        return rewards
        
    except Exception as e:
        print(f"   âš ï¸ å¤šæ ·æ€§è®¡ç®—å¤±è´¥: {e}")
        return torch.zeros(len(completions), dtype=torch.float32)

def reward_inter_sample_diversity_batch(completions, **kwargs):
    """æ‰¹æ¬¡å†…æ ·æœ¬å¤šæ ·æ€§å¥–åŠ±"""
    global current_training_step
    
    print(f"ğŸ”„ Step {current_training_step} - è¯„ä¼°æ‰¹æ¬¡å†…å¤šæ ·æ€§å¥–åŠ± (batch size: {len(completions)})")
    
    try:
        texts = [extract_text_content_global(completion) for completion in completions]
        texts = [text for text in texts if text and len(text) > 10]
        
        if len(texts) < 2:
            return torch.zeros(len(completions), dtype=torch.float32)
        
        # ä½¿ç”¨ç®€å•çš„jaccardè·ç¦»è®¡ç®—å¤šæ ·æ€§
        def jaccard_similarity(text1, text2):
            words1 = set(text1.lower().split())
            words2 = set(text2.lower().split())
            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))
            return intersection / union if union > 0 else 0.0
        
        # è®¡ç®—æ‰€æœ‰æ ·æœ¬å¯¹çš„å¤šæ ·æ€§
        diversities = []
        for i in range(len(texts)):
            for j in range(i + 1, len(texts)):
                similarity = jaccard_similarity(texts[i], texts[j])
                diversity = 1.0 - similarity
                diversities.append(diversity)
        
        if not diversities:
            return torch.zeros(len(completions), dtype=torch.float32)
        
        avg_diversity = np.mean(diversities)
        avg_diversity = float(avg_diversity)  # ç¡®ä¿æ˜¯æ ‡é‡
        
        # ä¸‹é¢çš„è®¡ç®—åº”å½“ä»åœ¨tryå—ä¸­ï¼Œå¦åˆ™tryåå°†ç¼ºå°‘exceptå¯¼è‡´è¯­æ³•é”™è¯¯
        raw_norm = max(0.0, min(1.0, avg_diversity))
        dyn_w = get_scaled_weight("diversity", current_training_step) if _DYNAMIC_AVAILABLE else 0.3
        continuous_reward = (raw_norm ** 1.3) * dyn_w
        legacy_batch_reward = (avg_diversity - 0.5) * 0.4  # åŸå§‹æ˜ å°„
        batch_reward = max(legacy_batch_reward, continuous_reward)
        ma = update_moving_average("diversity_pair_raw", raw_norm) if _DYNAMIC_AVAILABLE else raw_norm
        rewards = torch.full((len(completions),), float(batch_reward), dtype=torch.float32)
        
        print(f"   å¹³å‡Jaccardå¤šæ ·æ€§: {avg_diversity:.3f}, raw={raw_norm:.3f}, è¿ç»­={continuous_reward:.4f}, æœ€ç»ˆbatchå¥–åŠ±={batch_reward:.4f}, MA={ma:.3f}")
        if _DYNAMIC_AVAILABLE and tracer is not None and hasattr(tracer, "log"):
            tracer.log(current_training_step, task="amazon", component="inter_sample_diversity", raw_score=raw_norm, final_reward=float(batch_reward),
                         extra={"legacy_reward": legacy_batch_reward, "dyn_weight": dyn_w})
        
        # ä»kwargsä¸­ç§»é™¤stepå‚æ•°ï¼Œé¿å…é‡å¤ä¼ é€’
        kwargs_copy = kwargs.copy()
        kwargs_copy.pop('step', None)
        log_reward_details(current_training_step, "inter_sample_diversity", completions, rewards, **kwargs_copy)
        return rewards
        
    except Exception as e:
        print(f"   âš ï¸ æ‰¹æ¬¡å¤šæ ·æ€§è®¡ç®—å¤±è´¥: {e}")
        return torch.zeros(len(completions), dtype=torch.float32)

# =============================================================================
# ç»¼åˆå¥–åŠ±å‡½æ•°
# =============================================================================

def create_weighted_reward_functions():
    """åˆ›å»ºåŠ æƒçš„å¥–åŠ±å‡½æ•° - åŒ…å«Sampleçº§åˆ«å’ŒBatchçº§åˆ«çš„å¥–åŠ±"""
    
    def weighted_sentiment_consistency(completions, **kwargs):
        """åŠ æƒæƒ…æ„Ÿä¸€è‡´æ€§å¥–åŠ±å‡½æ•°"""
        global reward_call_counter, current_training_step
        
        # æ›´æ–°å…¨å±€è®¡æ•°å™¨å’Œè®­ç»ƒæ­¥æ•°
        reward_call_counter += 1
        current_training_step = reward_call_counter // 5  # æ¯5æ¬¡å¥–åŠ±å‡½æ•°è°ƒç”¨ä¸ºä¸€ä¸ªè®­ç»ƒæ­¥ï¼ˆç°åœ¨æœ‰5ä¸ªå¥–åŠ±å‡½æ•°ï¼‰
        
        # æ›´æ–°å½“å‰æ‰¹æ¬¡çš„æç¤ºå±æ€§
        update_current_prompt_attributes(current_training_step)
        
        base_rewards = reward_sentiment_consistency_batch(completions, **kwargs)
        return base_rewards  # è¿”å›æœªåŠ æƒçš„åŸºç¡€å¥–åŠ±
    
    def weighted_attribute_compliance(completions, **kwargs):
        """å±æ€§ç¬¦åˆåº¦å¥–åŠ±å‡½æ•°ï¼ˆè¿”å›æœªåŠ æƒå€¼ï¼‰"""
        base_rewards = reward_attribute_compliance_batch(completions, **kwargs)
        return base_rewards  # è¿”å›æœªåŠ æƒçš„åŸºç¡€å¥–åŠ±
    
    def weighted_length_compliance(completions, **kwargs):
        """é•¿åº¦ç¬¦åˆåº¦å¥–åŠ±å‡½æ•°ï¼ˆè¿”å›æœªåŠ æƒå€¼ï¼‰"""
        base_rewards = reward_length_compliance_batch(completions, **kwargs)
        return base_rewards  # è¿”å›æœªåŠ æƒçš„åŸºç¡€å¥–åŠ±
    
    def weighted_generation_quality(completions, **kwargs):
        """ç”Ÿæˆè´¨é‡å¥–åŠ±å‡½æ•°"""
        base_rewards = reward_generation_quality_batch(completions, **kwargs)
        return base_rewards  # è¿”å›æœªåŠ æƒçš„åŸºç¡€å¥–åŠ±
    
    def weighted_batch_diversity(completions, **kwargs):
        """Batchçº§åˆ«å¤šæ ·æ€§å¥–åŠ±å‡½æ•°"""
        if reward_batch_diversity is None:
            print("âš ï¸ Batchå¤šæ ·æ€§å¥–åŠ±å‡½æ•°ä¸å¯ç”¨ï¼Œè¿”å›ä¸­æ€§å¥–åŠ±")
            return [0.0] * len(completions)
        
        base_rewards = reward_batch_diversity(completions, **kwargs)
        return base_rewards  # è¿”å›æœªåŠ æƒçš„åŸºç¡€å¥–åŠ±
    
    # è®¾ç½®å‡½æ•°å
    weighted_sentiment_consistency.__name__ = "reward_sentiment_consistency"
    weighted_attribute_compliance.__name__ = "reward_attribute_compliance"
    weighted_length_compliance.__name__ = "reward_length_compliance"
    weighted_generation_quality.__name__ = "reward_generation_quality"
    weighted_batch_diversity.__name__ = "reward_batch_diversity"
    
    return [
        weighted_sentiment_consistency,    # Sampleçº§åˆ«: æƒ…æ„Ÿæ ‡ç­¾ä¸€è‡´æ€§
        weighted_attribute_compliance,     # Sampleçº§åˆ«: å±æ€§è¦æ±‚ç¬¦åˆåº¦  
        weighted_length_compliance,        # Sampleçº§åˆ«: é•¿åº¦è¦æ±‚ç¬¦åˆåº¦
        weighted_generation_quality,       # Sampleçº§åˆ«: ç”Ÿæˆè´¨é‡æ§åˆ¶
        weighted_batch_diversity,          # Batchçº§åˆ«: åŸºäºå±€éƒ¨å¯†åº¦çš„å¤šæ ·æ€§å¥–åŠ±
    ]

def save_training_logs(output_dir):
    """ä¿å­˜è®­ç»ƒæ—¥å¿—"""
    global reward_logs
    
    if reward_logs:
        logs_file = f"{output_dir}/reward_logs.json"
        import json
        with open(logs_file, 'w', encoding='utf-8') as f:
            json.dump(reward_logs, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“Š å¥–åŠ±æ—¥å¿—å·²ä¿å­˜: {logs_file}")

def initialize_reward_globals(td_global, bs_global, rc, nc, al, cc, optimized_sample_config=None, optimized_batch_config=None, embedding_model_path=None):
    """åˆå§‹åŒ–å¥–åŠ±å‡½æ•°æ¨¡å—çš„å…¨å±€å˜é‡ï¼ˆSample + Batchçº§åˆ«ï¼‰"""
    global training_data_global, batch_size_global, reward_calculator, novelsum_calculator, attr_loader, compliance_calculator
    global CURRENT_SAMPLE_REWARDS_CONFIG, CURRENT_BATCH_REWARDS_CONFIG
    
    training_data_global = td_global
    batch_size_global = bs_global
    reward_calculator = rc
    novelsum_calculator = nc  # ä¿ç•™å…¼å®¹æ€§ï¼Œä½†ä¸ä½¿ç”¨
    attr_loader = al
    compliance_calculator = cc
    
    # æ›´æ–°é…ç½®ï¼ˆå¦‚æœæä¾›äº†ä¼˜åŒ–é…ç½®ï¼‰
    if optimized_sample_config:
        CURRENT_SAMPLE_REWARDS_CONFIG.update(optimized_sample_config)
        print(f"âœ… ä½¿ç”¨ä¼˜åŒ–çš„Sampleå¥–åŠ±é…ç½®: {CURRENT_SAMPLE_REWARDS_CONFIG}")
    
    # æ›´æ–°batché…ç½®
    if optimized_batch_config:
        CURRENT_BATCH_REWARDS_CONFIG.update(optimized_batch_config)
        print(f"âœ… ä½¿ç”¨ä¼˜åŒ–çš„Batchå¥–åŠ±é…ç½®: {CURRENT_BATCH_REWARDS_CONFIG}")
    
    # åˆå§‹åŒ–batchå¤šæ ·æ€§è®¡ç®—å™¨
    if embedding_model_path and initialize_batch_diversity_calculator:
        try:
            initialize_batch_diversity_calculator(embedding_model_path, device='cuda', k_penalty=2.0)
            print("âœ… Batchå¤šæ ·æ€§è®¡ç®—å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ Batchå¤šæ ·æ€§è®¡ç®—å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    print("âœ… å¥–åŠ±å‡½æ•°æ¨¡å—å…¨å±€å˜é‡åˆå§‹åŒ–å®Œæˆï¼ˆSample + Batchçº§åˆ«ï¼‰")

def set_training_visualizer(visualizer):
    """è®¾ç½®è®­ç»ƒå¯è§†åŒ–å™¨"""
    global training_visualizer
    training_visualizer = visualizer
    print(f"âœ… è®­ç»ƒå¯è§†åŒ–å™¨å·²è®¾ç½®")