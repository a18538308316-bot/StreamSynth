#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Batchçº§åˆ«å¤šæ ·æ€§å¥–åŠ±å‡½æ•°
å®žçŽ°åŸºäºŽå±€éƒ¨å¯†åº¦çš„distinctivenessè¯„åˆ†
"""

import torch
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings("ignore")

class BatchDiversityCalculator:
    """Batchçº§åˆ«å¤šæ ·æ€§è®¡ç®—å™¨"""
    
    def __init__(self, embedding_model_path, device='cuda', k_penalty=2.0):
        """
        åˆå§‹åŒ–Batchå¤šæ ·æ€§è®¡ç®—å™¨
        
        Args:
            embedding_model_path: åµŒå…¥æ¨¡åž‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
            k_penalty: æŒ‡æ•°è¡°å‡çš„æƒ©ç½šå¼ºåº¦å‚æ•°
        """
        self.device = device
        self.k_penalty = k_penalty
        
        # åŠ è½½åµŒå…¥æ¨¡åž‹
        print(f"ðŸ”§ åˆå§‹åŒ–Batchå¤šæ ·æ€§è®¡ç®—å™¨...")
        self.embedding_model = SentenceTransformer(embedding_model_path)
        self.embedding_model = self.embedding_model.to(device)
        print(f"âœ… åµŒå…¥æ¨¡åž‹å·²åŠ è½½åˆ° {device}")
        
    def compute_embeddings(self, texts):
        """è®¡ç®—æ–‡æœ¬åµŒå…¥"""
        if not texts:
            return np.array([])
        
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
        texts = [str(text) for text in texts]
        
        with torch.no_grad():
            embeddings = self.embedding_model.encode(
                texts, 
                convert_to_tensor=True, 
                device=self.device,
                show_progress_bar=False
            )
            return embeddings.cpu().numpy()
    
    def calculate_local_density(self, target_embedding, batch_embeddings):
        """
        è®¡ç®—ç›®æ ‡æ ·æœ¬åœ¨batchä¸­çš„å±€éƒ¨å¯†åº¦
        
        Args:
            target_embedding: ç›®æ ‡æ ·æœ¬çš„åµŒå…¥å‘é‡ (1, d)
            batch_embeddings: batchä¸­æ‰€æœ‰æ ·æœ¬çš„åµŒå…¥å‘é‡ (m, d)
            
        Returns:
            local_density: å±€éƒ¨å¯†åº¦å€¼
        """
        if len(batch_embeddings) == 0:
            return 0.0
        
        # è®¡ç®—cosineç›¸ä¼¼åº¦
        similarities = cosine_similarity(target_embedding, batch_embeddings)[0]  # (m,)
        
        # è®¡ç®—proximityæƒé‡
        # w_j = sim(x_new, x_j) / sum(sim(x_new, x_k))
        similarity_sum = np.sum(similarities)
        if similarity_sum == 0:
            # é¿å…é™¤é›¶é”™è¯¯
            proximity_weights = np.ones(len(similarities)) / len(similarities)
        else:
            proximity_weights = similarities / similarity_sum
        
        # è®¡ç®—åŠ æƒå¹³å‡ç›¸ä¼¼åº¦ï¼ˆå±€éƒ¨å¯†åº¦ï¼‰
        local_density = np.sum(proximity_weights * similarities)
        
        return local_density
    
    def calculate_distinctiveness_score(self, local_density):
        """
        å°†å±€éƒ¨å¯†åº¦è½¬æ¢ä¸ºdistinctivenessè¯„åˆ†
        
        Args:
            local_density: å±€éƒ¨å¯†åº¦å€¼
            
        Returns:
            distinctiveness_score: ç‹¬ç‰¹æ€§è¯„åˆ† (0-1)
        """
        # ç®€åŒ–åŽçš„å¤šæ ·æ€§åˆ†æ•°ï¼š(1 - density) * 3ï¼Œå¹¶è£å‰ªåˆ°[0,1]
        # ä¹‹æ‰€ä»¥ä¹˜ä»¥3ï¼Œæ˜¯æ”¾å¤§åŽŸå§‹åˆ†æ•°ï¼Œä½¿å…¶å¯¹æœ€ç»ˆå¥–åŠ±æ›´æœ‰å½±å“åŠ›
        diversity_score = (1.0 - local_density) * 3.0
        # è£å‰ªåˆ°0-1åŒºé—´
        diversity_score = float(np.clip(diversity_score, 0.0, 1.0))
        return diversity_score
    
    def calculate_batch_diversity_rewards(self, completions):
        """
        è®¡ç®—batchä¸­æ¯ä¸ªæ ·æœ¬çš„å¤šæ ·æ€§å¥–åŠ±
        
        Args:
            completions: ç”Ÿæˆçš„å®Œæˆæ–‡æœ¬åˆ—è¡¨
            
        Returns:
            diversity_rewards: æ¯ä¸ªæ ·æœ¬çš„å¤šæ ·æ€§å¥–åŠ±åˆ—è¡¨
        """
        if not completions or len(completions) == 0:
            return []
        
        # å¦‚æžœåªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼Œç»™äºˆæœ€é«˜å¤šæ ·æ€§å¥–åŠ±
        if len(completions) == 1:
            return [1.0]
        
        try:
            # æ—©æœŸå¡Œé™·æ£€æµ‹ï¼šå…¨éƒ¨completionæžçŸ­æˆ–å…¨ç›¸åŒ => ç›´æŽ¥ç»™äºˆè½»åº¦æŽ¢ç´¢å¥–åŠ±
            lengths = [len(c.strip()) for c in completions]
            unique_texts = len({c.strip() for c in completions})
            if all(l <= 2 for l in lengths) or unique_texts == 1:
                print("âš ï¸ å¤šæ ·æ€§æ£€æµ‹: å…¨éƒ¨ç”ŸæˆæžçŸ­/å®Œå…¨ç›¸åŒï¼Œè¿”å›žæŽ¢ç´¢åŸºçº¿å¥–åŠ± 0.3")
                return [0.3] * len(completions)

            # è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„åµŒå…¥
            batch_embeddings = self.compute_embeddings(completions)
            if batch_embeddings.size == 0:
                return [0.0] * len(completions)

            diversity_rewards = []
            for i, _ in enumerate(completions):
                target_embedding = batch_embeddings[i:i+1]
                other_indices = list(range(len(completions)))
                other_indices.remove(i)
                other_embeddings = batch_embeddings[other_indices]
                local_density = self.calculate_local_density(target_embedding, other_embeddings)
                distinctiveness_score = self.calculate_distinctiveness_score(local_density)
                diversity_rewards.append(distinctiveness_score)

            # äºŒæ¬¡å¡Œé™·æ£€æµ‹ï¼šå…¨éƒ¨å¾—åˆ†ä¸º0åˆ™ä½¿ç”¨tokené›†åˆå·®å¼‚åº¦å›žé€€
            if max(diversity_rewards) == 0.0:
                print("âš ï¸ äºŒæ¬¡æ£€æµ‹: åµŒå…¥å±€éƒ¨å¯†åº¦å…¨éƒ¨å¯¼è‡´0åˆ†ï¼Œä½¿ç”¨fallbackåŸºäºŽtokenå·®å¼‚åº¦")
                import re
                token_sets = [set(re.findall(r"\w+", c.lower())) for c in completions]
                fallback_scores = []
                for i, ts in enumerate(token_sets):
                    others = [token_sets[j] for j in range(len(token_sets)) if j != i]
                    if not ts:
                        fallback_scores.append(0.0); continue
                    # è®¡ç®—ä¸Žå…¶ä»–é›†åˆçš„å¹³å‡Jaccardè·ç¦»
                    dists = []
                    for o in others:
                        if not o:
                            dists.append(0.0)
                        else:
                            inter = len(ts & o)
                            union = len(ts | o)
                            dists.append(1 - inter / union if union else 0.0)
                    avg_dist = sum(dists)/len(dists) if dists else 0.0
                    # å°†è·ç¦»æ˜ å°„åˆ° [0.2,1.0]ï¼Œä¿ç•™æŽ¢ç´¢æ¿€åŠ±
                    mapped = 0.2 + 0.8 * avg_dist
                    fallback_scores.append(float(np.clip(mapped, 0.0, 1.0)))
                diversity_rewards = fallback_scores
            return diversity_rewards
        except Exception as e:
            print(f"âš ï¸ Batchå¤šæ ·æ€§è®¡ç®—å¤±è´¥: {e}")
            return [0.5] * len(completions)
    
    def get_batch_diversity_stats(self, completions):
        """èŽ·å–batchå¤šæ ·æ€§ç»Ÿè®¡ä¿¡æ¯"""
        rewards = self.calculate_batch_diversity_rewards(completions)
        
        if not rewards:
            return {
                'mean_diversity': 0.0,
                'std_diversity': 0.0,
                'min_diversity': 0.0,
                'max_diversity': 0.0,
                'batch_size': 0
            }
        
        return {
            'mean_diversity': np.mean(rewards),
            'std_diversity': np.std(rewards),
            'min_diversity': np.min(rewards),
            'max_diversity': np.max(rewards),
            'batch_size': len(rewards)
        }

# å…¨å±€å˜é‡
batch_diversity_calculator = None

def initialize_batch_diversity_calculator(embedding_model_path, device='cuda', k_penalty=2.0):
    """åˆå§‹åŒ–å…¨å±€batchå¤šæ ·æ€§è®¡ç®—å™¨"""
    global batch_diversity_calculator
    batch_diversity_calculator = BatchDiversityCalculator(embedding_model_path, device, k_penalty)
    print("âœ… å…¨å±€Batchå¤šæ ·æ€§è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")

def reward_batch_diversity(completions, **kwargs):
    """
    Batchçº§åˆ«å¤šæ ·æ€§å¥–åŠ±å‡½æ•°
    
    Args:
        completions: ç”Ÿæˆçš„å®Œæˆæ–‡æœ¬åˆ—è¡¨
        
    Returns:
        rewards: æ¯ä¸ªæ ·æœ¬çš„å¤šæ ·æ€§å¥–åŠ±åˆ—è¡¨
    """
    global batch_diversity_calculator
    
    if batch_diversity_calculator is None:
        print("âš ï¸ Batchå¤šæ ·æ€§è®¡ç®—å™¨æœªåˆå§‹åŒ–ï¼Œè¿”å›žä¸­æ€§å¥–åŠ±")
        return [0.5] * len(completions)
    
    try:  # æ³¨æ„è¿™é‡Œçš„ç¼©è¿›ï¼Œåº”è¯¥ä¸Žifè¯­å¥åŒçº§
        # è®¡ç®—batchå¤šæ ·æ€§å¥–åŠ±ï¼ˆçŽ°åœ¨calculate_batch_diversity_rewardsè¿”å›žçš„æ˜¯
        # å·²æŒ‰ (1 - density) * 3 å¹¶è£å‰ªåˆ° [0,1] çš„åˆ†æ•°ï¼‰
        diversity_rewards = batch_diversity_calculator.calculate_batch_diversity_rewards(completions)

        # ç›´æŽ¥ä½¿ç”¨è¯¥åˆ†æ•°ä½œä¸ºå¤šæ ·æ€§å¥–åŠ±è¿”å›žï¼ˆä¸å†åšç¦»æ•£çš„æ¡¶æ˜ å°„ï¼‰
        stats = batch_diversity_calculator.get_batch_diversity_stats(completions)

        print(f"ðŸŽ¨ Batchå¤šæ ·æ€§å¥–åŠ± - å¹³å‡: {stats['mean_diversity']:.4f}, "
              f"æ ‡å‡†å·®: {stats['std_diversity']:.4f}, "
              f"èŒƒå›´: [{stats['min_diversity']:.4f}, {stats['max_diversity']:.4f}]")

        return diversity_rewards
        
    except Exception as e:
        print(f"âš ï¸ Batchå¤šæ ·æ€§å¥–åŠ±è®¡ç®—å¤±è´¥: {e}")
        return [0.0] * len(completions)

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    test_texts = [
        "This is a great Chinese restaurant with excellent service.",
        "I love the Italian food here, especially the pasta.",
        "The Mexican cuisine was amazing, very authentic flavors.",
        "Another Chinese restaurant review, but with different details."
    ]
    
    # åˆå§‹åŒ–è®¡ç®—å™¨
    embedding_model_path = "/public/home/huzhenlin2023/synthetic_data/all-MiniLM-L6-v2"
    calculator = BatchDiversityCalculator(embedding_model_path, device='cpu')
    
    # è®¡ç®—å¤šæ ·æ€§å¥–åŠ±
    rewards = calculator.calculate_batch_diversity_rewards(test_texts)
    stats = calculator.get_batch_diversity_stats(test_texts)
    
    print("æµ‹è¯•ç»“æžœ:")
    for i, (text, reward) in enumerate(zip(test_texts, rewards)):
        print(f"  æ ·æœ¬{i+1}: {reward:.4f} - {text[:50]}...")
    
    print(f"\nç»Ÿè®¡ä¿¡æ¯: {stats}")
