#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®å¤„ç†æ¨¡å— - æ•°æ®åŠ è½½ã€æ ¼å¼è½¬æ¢å’Œé¢„å¤„ç†
"""
import json
import os
from datasets import Dataset
from scripts_amazon.attribute_handler import extract_attributes_from_input

def load_synthesis_data(file_path, max_samples=None):
    """åŠ è½½åˆæˆæ•°æ®ï¼Œè½¬æ¢ä¸ºGRPOå…¼å®¹æ ¼å¼"""
    print(f"Loading synthesis data from {file_path}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if max_samples:
        data = data[:max_samples]
    
    formatted_data = []
    for item in data:
        # è§£ærequirementséƒ¨åˆ†ä»¥è·å–çœŸå®çš„ç›®æ ‡å±æ€§
        if 'instruction' in item and 'input' in item and 'output' in item:
            try:
                # ä»outputä¸­è§£æJSONè·å–çœŸå®çš„æƒ…æ„Ÿæ ‡ç­¾ï¼ˆAmazonæ•°æ®æ ¼å¼ï¼‰
                output_json = json.loads(item['output'])
                true_target_sentiment = output_json.get('output', 'neutral')  # è¿™æ˜¯çœŸå®çš„ç›®æ ‡æƒ…æ„Ÿ
                review_text = output_json.get('input', '').replace('Text: ', '')
                generated_sentiment = true_target_sentiment  # åœ¨Amazonæ•°æ®ä¸­ï¼Œoutputå°±æ˜¯ç›®æ ‡æƒ…æ„Ÿ
                
                # å¦‚æœoutputè§£æå¤±è´¥ï¼Œä»inputä¸­æå–ä½œä¸ºå…œåº•
                if true_target_sentiment == 'neutral':
                    requirements_text = item['input']
                    attributes = extract_attributes_from_input(requirements_text)
                    true_target_sentiment = attributes.get('target_sentiment', 'neutral')
                
                # æ„å»ºGRPOæ ¼å¼çš„messagesï¼ˆåªéœ€è¦promptï¼Œä¸éœ€è¦å‚è€ƒç­”æ¡ˆï¼‰
                user_message = f"{item['instruction']}\n\n{item['input']}"
                
                formatted_item = {
                    'messages': [
                        {'role': 'user', 'content': user_message}
                        # æ³¨æ„ï¼šGRPOä¸éœ€è¦assistantæ¶ˆæ¯ï¼Œæ¨¡å‹ä¼šè‡ªå·±ç”Ÿæˆ
                    ],
                    # å…³é”®ä¿¡æ¯ï¼šä½¿ç”¨requirementsä¸­çš„çœŸå®ç›®æ ‡
                    'sentiment': true_target_sentiment,  # çœŸå®ç›®æ ‡æƒ…æ„Ÿ
                    'generated_sentiment': generated_sentiment,  # GPTç”Ÿæˆçš„æƒ…æ„Ÿï¼ˆä¾›å¯¹æ¯”ï¼‰
                    'review_text': review_text,  # ç”Ÿæˆçš„è¯„è®ºæ–‡æœ¬
                    'original_input': item['input']  # ä¿å­˜åŸå§‹requirementsç”¨äºå±æ€§æå–
                }
                formatted_data.append(formatted_item)
                
            except (json.JSONDecodeError, KeyError) as e:
                print(f"âš ï¸ è·³è¿‡æ ¼å¼é”™è¯¯çš„æ ·æœ¬: {e}")
                continue
                
        # å…¼å®¹åŸæœ‰çš„messagesæ ¼å¼
        elif 'messages' in item and len(item['messages']) >= 1:
            # å¯¹äºå·²æœ‰çš„messagesæ ¼å¼ï¼Œæå–çœŸå®ç›®æ ‡æƒ…æ„Ÿ
            user_content = item['messages'][0].get('content', '')
            attributes = extract_attributes_from_input(user_content)
            
            formatted_item = {
                'messages': item['messages'],
                'sentiment': attributes.get('target_sentiment', 'neutral'),
                'original_input': user_content
            }
            
            # å¦‚æœæœ‰å…¶ä»–å­—æ®µï¼Œä¹Ÿä¿ç•™
            for key in ['review_text', 'generated_sentiment']:
                if key in item:
                    formatted_item[key] = item[key]
                    
            formatted_data.append(formatted_item)
        else:
            print(f"âš ï¸ è·³è¿‡ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {list(item.keys())}")
    
    print(f"Loaded {len(formatted_data)} samples")
    return formatted_data

def create_sentiment_grouped_dataset(data, batch_size=4):
    """åˆ›å»ºæŒ‰æƒ…æ„Ÿæ ‡ç­¾åˆ†ç»„çš„æ•°æ®é›†ï¼Œä¼˜åŒ–å°batchè®­ç»ƒ"""
    print(f"ğŸ”„ æŒ‰æƒ…æ„Ÿæ ‡ç­¾é‡ç»„æ•°æ®é›† (ç›®æ ‡batchå¤§å°: {batch_size})...")
    
    # æŒ‰æƒ…æ„Ÿæ ‡ç­¾åˆ†ç»„
    sentiment_groups = {
        'very negative': [],
        'negative': [],
        'neutral': [],
        'positive': [],
        'very positive': []
    }
    
    for item in data:
        sentiment = item.get('sentiment', 'neutral')
        if sentiment in sentiment_groups:
            sentiment_groups[sentiment].append(item)
        else:
            # å¤„ç†æœªçŸ¥æƒ…æ„Ÿæ ‡ç­¾
            print(f"âš ï¸ æœªçŸ¥æƒ…æ„Ÿæ ‡ç­¾: {sentiment}ï¼Œå½’ç±»ä¸ºneutral")
            sentiment_groups['neutral'].append(item)
                
    
    # æ‰“å°åˆ†ç»„ç»Ÿè®¡
    print("ğŸ“Š æƒ…æ„Ÿæ ‡ç­¾åˆ†ç»„ç»Ÿè®¡:")
    for sentiment, items in sentiment_groups.items():
        print(f"   {sentiment}: {len(items)}ä¸ªæ ·æœ¬")
    
    # é‡æ–°ç»„ç»‡æ•°æ®ï¼Œç¡®ä¿åŒä¸€batchå†…å°½å¯èƒ½æ˜¯åŒä¸€æ ‡ç­¾
    reorganized_data = []
    
    # ä¸ºæ¯ä¸ªæƒ…æ„Ÿæ ‡ç­¾åˆ›å»ºå®Œæ•´çš„batch
    for sentiment, items in sentiment_groups.items():
        if items:  # åªå¤„ç†éç©ºçš„ç»„
            # å°†è¯¥æƒ…æ„Ÿçš„æ ·æœ¬æŒ‰batch_sizeåˆ†ç»„
            for i in range(0, len(items), batch_size):
                batch_items = items[i:i + batch_size]
                reorganized_data.extend(batch_items)
                print(f"   æ·»åŠ {sentiment}æ‰¹æ¬¡: {len(batch_items)}ä¸ªæ ·æœ¬")
    
    print(f"âœ… æ•°æ®é‡ç»„å®Œæˆ: {len(reorganized_data)}ä¸ªæ ·æœ¬")
    print(f"ğŸ¯ é¢„æœŸbatchæ•°: {len(reorganized_data) // batch_size}")
    
    return reorganized_data

def prepare_grpo_dataset(data_list):
    """å‡†å¤‡GRPOè®­ç»ƒæ•°æ®é›†"""
    print("ğŸ”„ å‡†å¤‡GRPOæ•°æ®é›†...")
    
    # ä¸ºGRPOæ·»åŠ promptå­—æ®µï¼Œå¹¶åˆ é™¤messageså­—æ®µé¿å…å†²çª
    for item in data_list:
        if 'messages' in item and len(item['messages']) > 0:
            # æå–ç”¨æˆ·æ¶ˆæ¯ä½œä¸ºprompt
            user_message = item['messages'][0]['content']
            item['prompt'] = user_message
            # åˆ é™¤messageså­—æ®µé¿å…ä¸GRPOå†²çª
            del item['messages']
    
    dataset = Dataset.from_list(data_list)
    print(f"âœ… GRPOæ•°æ®é›†å‡†å¤‡å®Œæˆï¼Œå…±{len(dataset)}ä¸ªæ ·æœ¬")
    
    return dataset

def validate_dataset_format(dataset):
    """éªŒè¯æ•°æ®é›†æ ¼å¼"""
    print("ğŸ” éªŒè¯æ•°æ®æ ¼å¼...")
    sample_item = dataset[0]
    print(f"   æ•°æ®å­—æ®µ: {list(sample_item.keys())}")
    if 'prompt' in sample_item:
        print(f"   ç¤ºä¾‹prompté•¿åº¦: {len(sample_item['prompt'])}")
    if 'sentiment' in sample_item:
        print(f"   ç¤ºä¾‹æƒ…æ„Ÿæ ‡ç­¾: {sample_item['sentiment']}")
    print("âœ… æ•°æ®æ ¼å¼éªŒè¯å®Œæˆ")
    
    return True

def create_optimized_dataset(file_path, max_samples=None, batch_size=4):
    """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®é›†ï¼ˆå®Œæ•´æµç¨‹ï¼‰"""
    # åŠ è½½æ•°æ®
    data = load_synthesis_data(file_path, max_samples)
    print(f"ğŸ”„ åŸå§‹æ•°æ®åŠ è½½å®Œæˆï¼Œå…±{len(data)}ä¸ªæ ·æœ¬")
    
    # åº”ç”¨æƒ…æ„Ÿåˆ†ç»„ä¼˜åŒ–
    print("ğŸ§  åº”ç”¨æƒ…æ„Ÿåˆ†ç»„ä¼˜åŒ–...")
    grouped_data = create_sentiment_grouped_dataset(data, batch_size)
    print(f"âœ… æƒ…æ„Ÿåˆ†ç»„å®Œæˆï¼Œä¼˜åŒ–åæ•°æ®é‡: {len(grouped_data)}ä¸ªæ ·æœ¬")
    
    # å‡†å¤‡GRPOæ•°æ®é›†
    dataset = prepare_grpo_dataset(grouped_data)
    
    # éªŒè¯æ ¼å¼
    validate_dataset_format(dataset)
    
    return dataset, grouped_data