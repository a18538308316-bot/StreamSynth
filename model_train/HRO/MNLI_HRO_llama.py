#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¥–åŠ±å¢å¼ºç‰ˆGRPOè®­ç»ƒè„šæœ¬ - é’ˆå¯¹æ­£å‘å¥–åŠ±çš„ä¼˜åŒ– (MNLIæ•°æ®é›†)
åŸºäºproductionç‰ˆæœ¬åˆ†æï¼Œæ”¾å®½è¯„åˆ†æ ‡å‡†ï¼Œæé«˜å¥–åŠ±ä¸Šé™
"""

import os
import sys
import time
import json
import re
import torch
import numpy as np

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from trl import GRPOConfig, GRPOTrainer
from peft import LoraConfig
from datasets import Dataset

from scripts_MNLI.data_utils import create_optimized_dataset
from scripts_MNLI.reward_functions import initialize_reward_globals, set_training_visualizer
from scripts_MNLI.training_visualizer import initialize_visualizer
from scripts_MNLI.mnli_attribute_config import MNLI_ATTRPROMPT_CONFIG, load_mnli_sample_attributes

# =============================================================================
# å¥–åŠ±å¢å¼ºé…ç½®
# =============================================================================

DATA_FILE = "/public/home/huzhenlin2023/paper_2_LLM_Synthesis/synthesis_model_train/MNLI/MNLI_train_1496.json"
MERGED_MODEL_PATH = "/public/home/huzhenlin2023/paper_2_LLM_Synthesis/synthesis_model_train/TRL-GRPO-ohter-dataset/MNLI/merged_grpo_sft_mnli_model"
OUTPUT_DIR = "/public/home/huzhenlin2023/paper_2_LLM_Synthesis/synthesis_model_train/TRL-GRPO-ohter-dataset/MNLI/MNLI_sft_grpo_enhanced_output-sim500-epo2-bat4-gr4-gen4"

USE_MERGED_MODEL = True

READABILITY_MODEL_PATH = "/public/home/huzhenlin2023/paper_2_LLM_Synthesis/evaluate_model_data_continual_learning/reasoning-model"
EMBEDDING_MODEL_PATH = "/public/home/huzhenlin2023/synthetic_data/all-MiniLM-L6-v2"

ENHANCED_CONFIG = {
    "max_train_samples": 500,
    "num_train_epochs": 2.0,
    "per_device_train_batch_size": 4,
    "gradient_accumulation_steps": 4,
    "num_generations": 4,
    "max_completion_length": 800,
    "logging_steps": 5,
    "save_steps": 40,
    "learning_rate": 3e-6,
    "warmup_steps": 20,
    "max_grad_norm": 1.0,
    "dataloader_num_workers": 4,
}

ENHANCED_SAMPLE_REWARDS_CONFIG = {
    "label_consistency_weight": 0.1,
    "attribute_compliance_weight": 0.1,
    "generation_quality_weight": 0.5,
}

ENHANCED_BATCH_REWARDS_CONFIG = {
    "batch_diversity_weight": 0.3,
}

GENERATION_CONFIG = {
    "do_sample": True,
    "temperature": 0.85,
    "top_p": 0.92,
    "top_k": 60,
    "repetition_penalty": 1.12,
    "pad_token_id": None,
}
def create_enhanced_reward_functions():
    """åˆ›å»ºå¢å¼ºç‰ˆå¥–åŠ±å‡½æ•°ï¼Œè§£æprompt/completionä»¥è·å¾—çœŸå®æ ‡ç­¾ä¸å±æ€§"""
    from scripts_MNLI.reward_functions import reward_generation_quality_batch
    from scripts_MNLI.batch_diversity_reward import reward_batch_diversity

    mnli_labels = {'entailment', 'contradiction', 'neutral'}

    def _extract_first_json_object(text):
        decoder = json.JSONDecoder()
        idx = 0
        while idx < len(text):
            brace_idx = text.find('{', idx)
            if brace_idx == -1:
                break
            try:
                obj, end = decoder.raw_decode(text, brace_idx)
                if isinstance(obj, dict):
                    return obj
            except json.JSONDecodeError:
                pass
            idx = brace_idx + 1
        return None

    def _clean_nli_text(input_value):
        if not isinstance(input_value, str):
            return ""
        return input_value.strip()

    def _split_premise_hypothesis(text):
        if not text:
            return "", ""
        match = re.search(r'Premise:\s*(.*?)\s*Hypothesis:\s*(.*)', text, flags=re.IGNORECASE | re.DOTALL)
        if match:
            premise = match.group(1).strip()
            hypothesis = match.group(2).strip()
            return premise, hypothesis
        return text.strip(), ""

    def _extract_target_label(prompt):
        if not isinstance(prompt, str):
            return 'neutral'
        patterns = [
            r'Target label \(must match exactly\):\s*([^\n]+)',
            r'Target label:\s*([^\n]+)',
            r'label \(must match exactly\):\s*([^\n]+)',
            r'label:\s*([^\n]+)'
        ]
        for pattern in patterns:
            match = re.search(pattern, prompt, flags=re.IGNORECASE)
            if match:
                candidate = match.group(1).strip().lower()
                if candidate in mnli_labels:
                    return candidate
        return 'neutral'

    def _extract_attributes(prompt):
        attributes = load_mnli_sample_attributes(prompt or "")
        return {k: v for k, v in attributes.items() if isinstance(v, (str, dict)) and v}

    def _has_negation(text):
        if not text:
            return False
        return bool(re.search(r"\b(no|not|never|none|cannot|can't|n't)\b", text.lower()))

    def _token_overlap(a, b):
        a_tokens = set(re.findall(r'\w+', a.lower()))
        b_tokens = set(re.findall(r'\w+', b.lower()))
        if not b_tokens:
            return 0.0
        return len(a_tokens & b_tokens) / len(b_tokens)

    def _infer_label_from_text(premise, hypothesis):
        if not premise or not hypothesis:
            return 'neutral'
        premise_lower = premise.lower()
        hypothesis_lower = hypothesis.lower()
        if hypothesis_lower in premise_lower or _token_overlap(premise, hypothesis) > 0.7 and _has_negation(premise) == _has_negation(hypothesis):
            return 'entailment'
        premise_neg = _has_negation(premise)
        hypothesis_neg = _has_negation(hypothesis)
        if _token_overlap(premise, hypothesis) > 0.4 and premise_neg != hypothesis_neg:
            return 'contradiction'
        if any(word in hypothesis_lower for word in ['cannot', "can't", 'never', 'no ']) and not premise_neg:
            return 'contradiction'
        return 'neutral'

    def _score_label_alignment(target_label, json_label, premise, hypothesis):
        json_label = json_label.lower() if isinstance(json_label, str) else None
        if json_label not in mnli_labels:
            json_label = None
        inferred_label = _infer_label_from_text(premise, hypothesis)
        if inferred_label == target_label:
            return 1.0 if json_label == target_label else 0.8
        if json_label == target_label:
            return 0.45
        if inferred_label in mnli_labels and json_label in mnli_labels:
            if {inferred_label, target_label} <= {'neutral', 'entailment'} or {inferred_label, target_label} <= {'neutral', 'contradiction'}:
                return 0.25
        return 0.0

    reward_call_counter = 0
    current_training_step = 0

    def enhanced_label_consistency(completions, **kwargs):
        nonlocal reward_call_counter, current_training_step
        reward_call_counter += 1
        current_training_step = reward_call_counter // 4
        kwargs['step'] = current_training_step

        prompts = kwargs.get('prompts', [])
        rewards = []
        for idx, completion in enumerate(completions):
            prompt = prompts[idx] if idx < len(prompts) else ""
            target_label = _extract_target_label(prompt)
            parsed_json = _extract_first_json_object(completion)
            json_label = parsed_json.get('output') if parsed_json else None
            qa_text = _clean_nli_text(parsed_json.get('input')) if parsed_json else completion
            premise, hypothesis = _split_premise_hypothesis(qa_text)
            reward = _score_label_alignment(target_label, json_label, premise, hypothesis)
            rewards.append(float(np.clip(reward, 0.0, 1.0)))
        return rewards

    def enhanced_attribute_compliance(completions, **kwargs):
        kwargs['step'] = current_training_step
        prompts = kwargs.get('prompts', [])
        rewards = []
        for idx, completion in enumerate(completions):
            prompt = prompts[idx] if idx < len(prompts) else ""
            attributes = _extract_attributes(prompt)
            target_label = _extract_target_label(prompt)
            parsed_json = _extract_first_json_object(completion)
            qa_text = _clean_nli_text(parsed_json.get('input')) if parsed_json else completion
            premise, hypothesis = _split_premise_hypothesis(qa_text)
            if not attributes or not qa_text:
                rewards.append(0.0)
                continue
            total_score = 0.0
            count = 0
            for attr_name, target_value in attributes.items():
                if attr_name == 'target_label':
                    continue
                check_func = MNLI_ATTRPROMPT_CONFIG.get_attribute_check_function(
                    attr_name, target_value, label=target_label
                )
                try:
                    if attr_name == 'length_premise':
                        attr_input = premise
                    elif attr_name == 'length_hypothesis':
                        attr_input = hypothesis
                    else:
                        attr_input = qa_text
                    attr_score = float(check_func(attr_input))
                except Exception:
                    attr_score = 0.0
                total_score += max(0.0, min(attr_score, 1.0))
                count += 1
            avg_score = total_score / count if count else 0.0
            if avg_score >= 0.75:
                reward = 1.0
            elif avg_score >= 0.55:
                reward = 0.7
            elif avg_score >= 0.35:
                reward = 0.4
            elif avg_score >= 0.15:
                reward = 0.15
            else:
                reward = 0.0
            rewards.append(float(reward))
        return rewards

    def enhanced_generation_quality(completions, **kwargs):
        kwargs['step'] = current_training_step
        base_rewards = reward_generation_quality_batch(completions, **kwargs)
        if hasattr(base_rewards, 'tolist'):
            return base_rewards.tolist()
        return [float(r) for r in base_rewards]

    def enhanced_batch_diversity(completions, **kwargs):
        kwargs['step'] = current_training_step
        rewards = reward_batch_diversity(completions, **kwargs)
        if hasattr(rewards, 'tolist'):
            return rewards.tolist()
        return [float(r) for r in rewards]

    enhanced_label_consistency.__name__ = "reward_label_consistency"
    enhanced_attribute_compliance.__name__ = "reward_attribute_compliance"
    enhanced_generation_quality.__name__ = "reward_generation_quality"
    enhanced_batch_diversity.__name__ = "reward_batch_diversity"

    return [
        enhanced_label_consistency,
        enhanced_attribute_compliance,
        enhanced_generation_quality,
        enhanced_batch_diversity,
    ]

# =============================================================================
# æ•°æ®å¤„ç†å‡½æ•°
# =============================================================================

def load_and_process_data(data_file, max_samples=None):
    """åŠ è½½å’Œå¤„ç†è®­ç»ƒæ•°æ®"""
    print(f"Loading synthesis data from {data_file}")
    
    with open(data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if max_samples:
        data = data[:max_samples]
    
    print(f"Loaded {len(data)} samples")
    
    # å‡†å¤‡GRPOæ•°æ®é›†
    grpo_data = []
    for item in data:
        full_prompt = item.get('instruction', '') + '\n\n' + item.get('input', '')
        
        input_text = item.get('input', '')
        output_text = item.get('output', '')
        target_label = 'neutral'  # é»˜è®¤æ ‡ç­¾
        
        # æ–¹æ³•1ï¼šä»outputå­—æ®µçš„JSONä¸­æå–æ ‡ç­¾ï¼ˆæœ€å‡†ç¡®ï¼‰
        try:
            import json
            # è§£æoutputä¸­çš„JSON
            output_json = json.loads(output_text)
            if 'output' in output_json:
                target_label = output_json['output']
        except (json.JSONDecodeError, KeyError, TypeError):
            # JSONè§£æå¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨å…¶ä»–æ–¹æ³•
            pass
        
        # æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1å¤±è´¥ï¼Œä»inputå­—æ®µä¸­æå–
        if target_label == 'neutral':
            if 'Target label (must match exactly):' in input_text:
                # æå–Target labelåé¢çš„å†…å®¹
                label_line = input_text.split('Target label (must match exactly):')[1].split('\n')[0].strip()
                target_label = label_line
        
        # éªŒè¯æ ‡ç­¾æ˜¯å¦æœ‰æ•ˆ
        if target_label not in ['entailment', 'contradiction', 'neutral']:
            target_label = 'neutral'  # æ— æ•ˆæ ‡ç­¾é»˜è®¤ä¸ºneutral
        
        grpo_item = {
            'label': target_label,
            'generated_label': target_label,
            'nli_text': item.get('output', ''),
            'original_input': item.get('input', ''),
            'prompt': full_prompt
        }
        grpo_data.append(grpo_item)
    
    print(f"âœ… GRPOæ•°æ®é›†å‡†å¤‡å®Œæˆï¼Œå…±{len(grpo_data)}ä¸ªæ ·æœ¬")
    return Dataset.from_list(grpo_data)

class SynthesisRewardCalculator:
    """æ•°æ®åˆæˆä»»åŠ¡çš„å¥–åŠ±è®¡ç®—å™¨"""
    
    def __init__(self, readability_model_path, device='cuda'):
        self.device = device
        self.readability_model_path = readability_model_path
        print(f"âœ… SynthesisRewardCalculatoråˆå§‹åŒ–å®Œæˆ (è®¾å¤‡: {device})")

def setup_reward_calculators():
    """è®¾ç½®MNLIæ•°æ®é›†ä¸“ç”¨å¥–åŠ±è®¡ç®—å™¨"""
    print("ğŸ”§ åˆå§‹åŒ–MNLIæ•°æ®é›†å¥–åŠ±å¢å¼ºç³»ç»Ÿ...")
    
    # ä½¿ç”¨MNLIä¸“ç”¨å±æ€§é…ç½®
    mnli_config = MNLI_ATTRPROMPT_CONFIG
    reward_calculator = SynthesisRewardCalculator(READABILITY_MODEL_PATH)
    
    print("âœ… MNLIå¥–åŠ±å¢å¼ºè®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ")
    print(f"ğŸ“‹ æ”¯æŒçš„å±æ€§: {list(mnli_config.attributes.keys())}")
    print(f"ğŸ“‹ æ”¯æŒçš„æ ‡ç­¾: {mnli_config.labels}")
    
    return reward_calculator, None, mnli_config, None

def plot_enhanced_training_curves(trainer, output_dir):
    """ç»˜åˆ¶å¢å¼ºç‰ˆè®­ç»ƒæ›²çº¿"""
    import matplotlib.pyplot as plt
    import pandas as pd
    
    try:
        log_history = trainer.state.log_history
        
        if not log_history:
            print("âš ï¸ No training logs found for plotting")
            return
        
        # æå–æ•°æ®
        steps = []
        total_rewards = []
        losses = []
        label_rewards = []
        attribute_rewards = []
        quality_rewards = []
        diversity_rewards = []
        
        for log_entry in log_history:
            if 'step' in log_entry:
                steps.append(log_entry['step'])
                total_rewards.append(log_entry.get('reward', 0))
                losses.append(log_entry.get('loss', 0))
                
                label_rewards.append(log_entry.get('rewards/reward_label_consistency/mean', 0))
                attribute_rewards.append(log_entry.get('rewards/reward_attribute_compliance/mean', 0))
                quality_rewards.append(log_entry.get('rewards/reward_generation_quality/mean', 0))
                diversity_rewards.append(log_entry.get('rewards/reward_batch_diversity/mean', 0))
        
        if not steps:
            print("âš ï¸ No step data found for plotting")
            return
        
        # è®¾ç½®å›¾è¡¨æ ·å¼ (æ”¹ä¸º2x2å¸ƒå±€ï¼Œå› ä¸ºåªæœ‰4ä¸ªå¥–åŠ±å‡½æ•°)
        plt.style.use('default')
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Reward Enhanced GRPO Training Progress (4 Rewards) - MNLI', fontsize=16, fontweight='bold')
        
        # 1. æ€»å¥–åŠ±æ›²çº¿
        axes[0, 0].plot(steps, total_rewards, 'b-', linewidth=2, marker='o', markersize=3)
        axes[0, 0].set_title('Total Reward (Enhanced)', fontweight='bold')
        axes[0, 0].set_xlabel('Training Step')
        axes[0, 0].set_ylabel('Reward')
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. æŸå¤±æ›²çº¿
        axes[0, 1].plot(steps, losses, 'r-', linewidth=2, marker='s', markersize=3)
        axes[0, 1].set_title('Training Loss', fontweight='bold')
        axes[0, 1].set_xlabel('Training Step')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. æ ‡ç­¾ä¸€è‡´æ€§å¥–åŠ±
        axes[0, 2].plot(steps, label_rewards, 'g-', linewidth=2, marker='^', markersize=3)
        axes[0, 2].set_title('Label Consistency (Max: 1.0)', fontweight='bold')
        axes[0, 2].set_xlabel('Training Step')
        axes[0, 2].set_ylabel('Reward')
        axes[0, 2].set_ylim(0, 1.1)  # è®¾ç½®yè½´èŒƒå›´
        axes[0, 2].grid(True, alpha=0.3)
        
        # 4. å±æ€§ç¬¦åˆåº¦å¥–åŠ±
        axes[1, 0].plot(steps, attribute_rewards, 'm-', linewidth=2, marker='d', markersize=3)
        axes[1, 0].set_title('Attribute Compliance (Max: 1.0)', fontweight='bold')
        axes[1, 0].set_xlabel('Training Step')
        axes[1, 0].set_ylabel('Reward')
        axes[1, 0].set_ylim(0, 1.1)  # è®¾ç½®yè½´èŒƒå›´
        axes[1, 0].grid(True, alpha=0.3)
        
        # 5. ç”Ÿæˆè´¨é‡å¥–åŠ± (åŒ…å«é•¿åº¦)
        axes[1, 1].plot(steps, quality_rewards, 'orange', linewidth=2, marker='*', markersize=4)
        axes[1, 1].set_title('Generation Quality (Max: 1.0, includes length)', fontweight='bold')
        axes[1, 1].set_xlabel('Training Step')
        axes[1, 1].set_ylabel('Reward')
        axes[1, 1].set_ylim(0, 1.1)  # è®¾ç½®yè½´èŒƒå›´
        axes[1, 1].grid(True, alpha=0.3)
        
        # 6. æ‰¹æ¬¡å¤šæ ·æ€§å¥–åŠ±
        axes[1, 2].plot(steps, diversity_rewards, 'cyan', linewidth=2, marker='v', markersize=3)
        axes[1, 2].set_title('Batch Diversity (Max: 1.0, lowered threshold)', fontweight='bold')
        axes[1, 2].set_xlabel('Training Step')
        axes[1, 2].set_ylabel('Reward')
        axes[1, 2].set_ylim(0, 1.1)  # è®¾ç½®yè½´èŒƒå›´
        axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        output_path = f"{output_dir}/enhanced_training_curves_mnli.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… Enhanced training curves saved to: {output_path}")
        
        # ä¿å­˜æ•°æ®åˆ°CSV
        df = pd.DataFrame({
            'Step': steps,
            'Total_Reward': total_rewards,
            'Loss': losses,
            'Label_Reward': label_rewards,
            'Attribute_Reward': attribute_rewards,
            'Quality_Reward': quality_rewards,
            'Diversity_Reward': diversity_rewards
        })
        
        csv_path = f"{output_dir}/enhanced_training_metrics_mnli.csv"
        df.to_csv(csv_path, index=False)
        print(f"âœ… Enhanced training metrics saved to: {csv_path}")
        
    except Exception as e:
        print(f"âŒ Error plotting enhanced training curves: {e}")
        import traceback
        traceback.print_exc()

def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    print("ğŸŒŸ å¯åŠ¨MNLIæ•°æ®é›†å¥–åŠ±å¢å¼ºç‰ˆGRPOè®­ç»ƒ...")
    print("ğŸ¯ æ ¸å¿ƒç­–ç•¥: é€‚é…MNLI NLIç”Ÿæˆï¼Œæ”¾å®½è¯„åˆ†æ ‡å‡†ï¼Œæé«˜å¥–åŠ±ä¸Šé™ï¼Œä¿ƒè¿›å­¦ä¹ ")
    print("ğŸ“Š é…ç½®: {}æ ·æœ¬, {}è½®".format(ENHANCED_CONFIG["max_train_samples"], ENHANCED_CONFIG["num_train_epochs"]))
    print("ğŸ¢ æ•°æ®é›†: MNLIè‡ªç„¶è¯­è¨€æ¨ç†")
    print("ğŸ·ï¸  æ ‡ç­¾: 3ä¸ªMNLIåˆ†ç±»æ ‡ç­¾")
    print("âš™ï¸  å±æ€§çº¦æŸ: 8ä¸ªMNLIç‰¹æœ‰å±æ€§ï¼ˆå‰æåŸŸã€æ¨ç†ç±»å‹ã€è¯­ä¹‰ç°è±¡ç­‰ï¼‰")
    
    print("ğŸ¯ å¥–åŠ±å¢å¼ºç­–ç•¥:")
    print(f"   æ ‡ç­¾ä¸€è‡´æ€§: ä¸Šé™1.0 (å®Œå…¨åŒ¹é…ç»™é«˜åˆ†)")
    print(f"   å±æ€§ç¬¦åˆåº¦: ä¸Šé™1.0 (æœ‰å…³é”®è¯å°±ç»™é«˜åˆ†)")
    print(f"   ç”Ÿæˆè´¨é‡: ä¸Šé™1.0 (åŒ…å«é•¿åº¦å’ŒNLIæ ¼å¼æ£€æŸ¥)")
    print(f"   æ‰¹æ¬¡å¤šæ ·æ€§: ä¸Šé™1.0 (å¤§å¹…é™ä½é˜ˆå€¼ï¼Œé€‚åº”å›ºå®šæç¤ºè¯)")
    
    print("ğŸ”§ è®­ç»ƒä¼˜åŒ–:")
    print(f"   ç”Ÿæˆå€™é€‰æ•°: {ENHANCED_CONFIG['num_generations']} (å¢åŠ é€‰æ‹©)")
    print(f"   å­¦ä¹ ç‡: {ENHANCED_CONFIG['learning_rate']} (ç¨å¾®æé«˜)")
    print(f"   é¢„çƒ­æ­¥æ•°: {ENHANCED_CONFIG['warmup_steps']} (å¢åŠ é¢„çƒ­)")
    print(f"   æ¸©åº¦: {GENERATION_CONFIG['temperature']} (ç¨å¾®æé«˜åˆ›é€ æ€§)")
    
    # åˆå§‹åŒ–è®­ç»ƒå¯è§†åŒ–å™¨
    visualizer = initialize_visualizer(OUTPUT_DIR)
    
    # è®¾ç½®å¥–åŠ±è®¡ç®—å™¨
    reward_calculator, novelsum_calculator, attr_loader, compliance_calculator = setup_reward_calculators()
    
    # åˆ›å»ºæ•°æ®é›†
    dataset, training_data_global = create_optimized_dataset(
        DATA_FILE, 
        ENHANCED_CONFIG['max_train_samples'], 
        ENHANCED_CONFIG['per_device_train_batch_size']
    )
    
    # åˆå§‹åŒ–MNLIæ•°æ®é›†å¥–åŠ±å‡½æ•°æ¨¡å—çš„å…¨å±€å˜é‡
    initialize_reward_globals(
        training_data_global, 
        ENHANCED_CONFIG['per_device_train_batch_size'],
        reward_calculator, 
        novelsum_calculator,
        attr_loader,  # è¿™é‡Œæ˜¯mnli_config
        compliance_calculator,  # è¿™é‡Œæ˜¯None
        optimized_sample_config=ENHANCED_SAMPLE_REWARDS_CONFIG,
        optimized_batch_config=ENHANCED_BATCH_REWARDS_CONFIG,
        embedding_model_path=EMBEDDING_MODEL_PATH
    )
    
    set_training_visualizer(visualizer)
    
    # æ£€æŸ¥å¹¶ä½¿ç”¨é¢„å…ˆåˆå¹¶çš„æ¨¡å‹
    if USE_MERGED_MODEL and os.path.exists(MERGED_MODEL_PATH):
        print("ğŸ¤– åŠ è½½é¢„å…ˆåˆå¹¶çš„å®Œæ•´SFTæ¨¡å‹...")
        
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            MERGED_MODEL_PATH,
            quantization_config=bnb_config,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        print("âœ… é¢„å…ˆåˆå¹¶çš„å®Œæ•´SFTæ¨¡å‹åŠ è½½æˆåŠŸ")
        
    else:
        print("âŒ é¢„å…ˆåˆå¹¶çš„æ¨¡å‹ä¸å­˜åœ¨!")
        return
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    tokenizer.padding_side = "left"
    
    # è®¾ç½®ç”Ÿæˆé…ç½®
    if hasattr(model, 'generation_config'):
        model.generation_config.do_sample = True
        model.generation_config.top_p = GENERATION_CONFIG["top_p"]
        model.generation_config.top_k = GENERATION_CONFIG["top_k"]
        model.generation_config.repetition_penalty = GENERATION_CONFIG["repetition_penalty"]
        model.generation_config.temperature = GENERATION_CONFIG["temperature"]
        model.generation_config.max_new_tokens = ENHANCED_CONFIG["max_completion_length"]
        model.generation_config.pad_token_id = tokenizer.eos_token_id
        model.generation_config.eos_token_id = tokenizer.eos_token_id
        print("ğŸ”¥ å¥–åŠ±å¢å¼ºç”Ÿæˆé…ç½®å®Œæˆ")
    
    # åˆ›å»ºå¢å¼ºç‰ˆå¥–åŠ±å‡½æ•°
    reward_functions = create_enhanced_reward_functions()
    print("âœ… å¥–åŠ±å¢å¼ºå‡½æ•°åˆ›å»ºå®Œæˆ")
    
    reward_weights = [
        ENHANCED_SAMPLE_REWARDS_CONFIG['label_consistency_weight'],
        ENHANCED_SAMPLE_REWARDS_CONFIG['attribute_compliance_weight'],
        ENHANCED_SAMPLE_REWARDS_CONFIG['generation_quality_weight'],
        ENHANCED_BATCH_REWARDS_CONFIG['batch_diversity_weight'],
    ]
    
    print(f"ğŸ“Š å¢å¼ºç‰ˆæƒé‡: {reward_weights}")
    print(f"ğŸ“Š æƒé‡æ€»å’Œ: {sum(reward_weights)}")
    
    # LoRAé…ç½®
    lora_config = LoraConfig(
        r=8, lora_alpha=16,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.1, bias="none", task_type="CAUSAL_LM", inference_mode=False
    )
    
    # é…ç½®GRPOè®­ç»ƒå‚æ•°
    grpo_config = GRPOConfig(
        learning_rate=ENHANCED_CONFIG["learning_rate"],
        num_train_epochs=ENHANCED_CONFIG["num_train_epochs"],
        per_device_train_batch_size=ENHANCED_CONFIG["per_device_train_batch_size"],
        gradient_accumulation_steps=ENHANCED_CONFIG["gradient_accumulation_steps"],
        logging_steps=ENHANCED_CONFIG["logging_steps"],
        save_steps=ENHANCED_CONFIG["save_steps"],
        warmup_steps=ENHANCED_CONFIG["warmup_steps"],
        max_grad_norm=ENHANCED_CONFIG["max_grad_norm"],
        dataloader_num_workers=ENHANCED_CONFIG["dataloader_num_workers"],
        output_dir=OUTPUT_DIR,
        num_generations=ENHANCED_CONFIG["num_generations"],
        max_completion_length=ENHANCED_CONFIG["max_completion_length"],
        reward_weights=reward_weights,
        temperature=GENERATION_CONFIG["temperature"],
        top_p=GENERATION_CONFIG["top_p"],
        top_k=GENERATION_CONFIG["top_k"],
        repetition_penalty=GENERATION_CONFIG["repetition_penalty"],
        remove_unused_columns=False,
        report_to=[],
    )
    
    # åˆå§‹åŒ–GRPOè®­ç»ƒå™¨
    trainer = GRPOTrainer(
        model=model,
        reward_funcs=reward_functions,
        args=grpo_config,
        train_dataset=dataset,
        processing_class=tokenizer,
        peft_config=lora_config,
    )
    
    print("ğŸ”¥ å¼€å§‹å¥–åŠ±å¢å¼ºGRPOè®­ç»ƒ...")
    print("ğŸ“‹ å¢å¼ºç­–ç•¥:")
    print("   - ğŸ¯ æ”¾å®½æ‰€æœ‰å¥–åŠ±çš„è¯„åˆ†æ ‡å‡†")
    print("   - ğŸ“ˆ æé«˜å¥–åŠ±ä¸Šé™ï¼Œç»™äºˆæ›´å¤šæ¿€åŠ±")
    print("   - ğŸ† å¢åŠ åŸºç¡€å¥–åŠ±ï¼Œå‡å°‘é›¶åˆ†æƒ…å†µ")
    print("   - ğŸš€ ä¼˜åŒ–è®­ç»ƒå‚æ•°ï¼Œæé«˜å­¦ä¹ æ•ˆç‡")
    print("   - ğŸ¨ å¢åŠ ç”Ÿæˆå¤šæ ·æ€§å’Œåˆ›é€ æ€§")
    print("-" * 80)
    
    # å¼€å§‹è®­ç»ƒ
    start_time = time.time()
    trainer.train()
    end_time = time.time()
    
    training_time = (end_time - start_time) / 60
    print(f"â±ï¸ è®­ç»ƒè€—æ—¶: {training_time:.2f}åˆ†é’Ÿ")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("ğŸ’¾ ä¿å­˜è®­ç»ƒåçš„æ¨¡å‹...")
    trainer.save_model()
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    print("ğŸ“Š ç»˜åˆ¶å¥–åŠ±å¢å¼ºè®­ç»ƒæ›²çº¿...")
    plot_enhanced_training_curves(trainer, OUTPUT_DIR)
    
    print("\n" + "=" * 80)
    print("ğŸ‰ MNLIæ•°æ®é›†å¥–åŠ±å¢å¼ºGRPOè®­ç»ƒå®Œæˆï¼")
    print(f"   è®­ç»ƒæ—¶é•¿: {training_time:.2f}åˆ†é’Ÿ")
    print(f"   å¤„ç†æ ·æœ¬: {ENHANCED_CONFIG['max_train_samples']}")
    print(f"   è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print("   MNLIä¸“ç”¨å¥–åŠ±ç‰¹æ€§:")
    print("     - âœ… æ ‡ç­¾ä¸€è‡´æ€§ï¼šåŸºäº3ä¸ªMNLIåˆ†ç±»æ ‡ç­¾ç²¾ç¡®åŒ¹é…")
    print("     - âœ… å±æ€§ç¬¦åˆåº¦ï¼š8ä¸ªMNLIå±æ€§çº¦æŸæ£€æŸ¥")
    print("     - âœ… ç”Ÿæˆè´¨é‡ï¼šMNLI NLIç‰¹ç‚¹å’Œè¯­è¨€é£æ ¼è¯„ä¼°")
    print("     - âœ… å¤šæ ·æ€§å¥–åŠ±ï¼šæ‰¹æ¬¡å†…å®¹å¤šæ ·æ€§è¯„ä¼°")
    print("     - âœ… ç»Ÿä¸€å¥–åŠ±ä¸Šé™ä¸º1.0ï¼Œå®½æ¾è¯„åˆ†æ ‡å‡†ä¿ƒè¿›å­¦ä¹ ")
    print("=" * 80)

if __name__ == "__main__":
    main()