#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Reward function module - contains all sample-level and batch-level reward functions.
"""
import torch
import numpy as np
from datetime import datetime
# Remove novelsum-specific imports but keep required utility functions

# Import batch diversity reward module
try:
    from .batch_diversity_reward import reward_batch_diversity, initialize_batch_diversity_calculator
    print("âœ… Batch diversity reward module imported successfully")
except ImportError as e:
    print(f"âš ï¸ Failed to import batch diversity reward module: {e}")
    reward_batch_diversity = None
    initialize_batch_diversity_calculator = None

# Global variables (populated from the main training module)
training_data_global = []
current_batch_index = 0
batch_size_global = 8
reward_calculator = None
novelsum_calculator = None
attr_loader = None
compliance_calculator = None
generation_history = []
reward_logs = []
current_training_step = 0
reward_call_counter = 0
current_prompt_attributes = {}
training_visualizer = None  # Training visualizer

# Default configuration for sample-level reward weights
SAMPLE_REWARDS_CONFIG = {
    'label_consistency_weight': 0.25,  # Label consistency
    'attribute_compliance_weight': 0.15,   # Attribute compliance
    'length_compliance_weight': 0.10,      # Length compliance
}

# Default configuration for batch-level reward weights
BATCH_REWARDS_CONFIG = {
    'batch_diversity_weight': 0.30,  # Batch diversity reward based on local density
}

# Optimized configurations (dynamically updated during training)
CURRENT_SAMPLE_REWARDS_CONFIG = SAMPLE_REWARDS_CONFIG.copy()
CURRENT_BATCH_REWARDS_CONFIG = BATCH_REWARDS_CONFIG.copy()

# =============================================================================
# Utility functions (migrated from the novelsum module where necessary)
# =============================================================================

def extract_text_content_global(text):
    """Global helper to extract and clean text content (simplified version)."""
    if not text:
        return ""
    
    # Basic text cleanup
    text = str(text).strip()
    
    # Remove extra whitespace characters
    import re
    text = re.sub(r'\s+', ' ', text)
    
    return text

def separate_prompt_and_generation_global(completion, prompt=""):
    """Global helper to separate prompt and generation (simplified version)."""
    if not completion:
        return "", ""
    
    completion = str(completion).strip()
    prompt = str(prompt).strip()
    
    # If a prompt is provided, try to strip it from completion
    if prompt and completion.startswith(prompt):
        generation = completion[len(prompt):].strip()
        return prompt, generation
    
    # If prompt is missing or cannot be separated, treat full completion as generation
    return prompt, completion

def get_current_batch_attributes(step, batch_size):
    """Get attribute information for the current batch based on training step."""
    global training_data_global, current_batch_index
    
    if not training_data_global:
        print("âš ï¸ Training data is not initialized, using default attributes")
        return {'target_label': 'Society & Culture', 'length': 200}
    
    # Use cyclic access to avoid going out of range
    total_samples = len(training_data_global)
    
    # Use modulo to ensure index stays in range
    start_idx = (step * batch_size) % total_samples
    
    # If we are looping over data again, log a message but do not raise
    if step * batch_size >= total_samples:
        epoch_num = (step * batch_size) // total_samples + 1
        if step % 50 == 0:  # Log every 50 steps to avoid excessive logs
            print(f"ğŸ”„ Training enters epoch {epoch_num}, step {step}, using cyclic data access")
    
    # Get original_input of the first sample in the current batch
    try:
        sample = training_data_global[start_idx]
        if 'original_input' in sample:
            from scripts_yahoo.attribute_handler import extract_attributes_from_input
            attributes = extract_attributes_from_input(sample['original_input'])
            print(f"ğŸ¯ Step {step}: extracted attributes from data - {attributes.get('target_label', 'unknown')}")
            return attributes
        else:
            print(f"âš ï¸ Sample {start_idx} is missing 'original_input' field")
            return {'target_label': 'Society & Culture', 'length': 200}
    except Exception as e:
        print(f"âš ï¸ Error while getting batch attributes: {e}")
        return {'target_label': 'Society & Culture', 'length': 200}

def extract_attributes_from_current_prompts(prompts):
    """Extract target attributes directly from the current prompts."""
    if not prompts or len(prompts) == 0:
        return {}
    
    # Use the first prompt as representative (same requirements within a batch)
    first_prompt = prompts[0]
    
    # Extract attributes from the prompt
    from scripts_yahoo.attribute_handler import extract_attributes_from_input
    
    # Directly extract attributes from prompt text
    attributes = extract_attributes_from_input(first_prompt)
    
    return attributes

def update_current_prompt_attributes_from_prompts(prompts):
    """Update current prompt attributes from the given prompts (preferred way)."""
    global current_prompt_attributes
    
    if prompts:
        current_prompt_attributes = extract_attributes_from_current_prompts(prompts)
        print(f"ğŸ¯ Extracted attributes from prompts: label={current_prompt_attributes.get('target_label', 'unknown')}")
    else:
        print("âš ï¸ No prompts provided, cannot extract attributes")

def update_current_prompt_attributes(step=None, batch_size=None):
    """Update current batch attributes (deprecated; prefer extracting from prompts)."""
    global current_prompt_attributes, current_batch_index, batch_size_global
    
    if step is not None and batch_size is not None:
        current_prompt_attributes = get_current_batch_attributes(step, batch_size)
        print(f"ğŸ”„ Updating prompt attributes from historical data (may be imprecise): {current_prompt_attributes}")
    elif step is not None:
        current_prompt_attributes = get_current_batch_attributes(step, batch_size_global)
        print(f"ğŸ”„ Updating prompt attributes using default batch_size")
    else:
        print("âš ï¸ Step is not provided, cannot update prompt attributes")

def log_reward_details(step, reward_type, completions, rewards, **kwargs):
    global reward_logs, training_visualizer
    
    sample_details = []
    for i, (completion, reward) in enumerate(zip(completions[:3], rewards[:3])):
        sample_details.append({
            'completion_preview': completion[:100],
            'reward': float(reward),
            'completion_length': len(completion)
        })
    
    reward_log = {
        'step': step,
        'reward_type': reward_type,
        'mean_reward': float(torch.mean(rewards)),
        'std_reward': float(torch.std(rewards)),
        'min_reward': float(torch.min(rewards)),
        'max_reward': float(torch.max(rewards)),
        'num_samples': len(rewards),
        'sample_details': sample_details,
        'timestamp': datetime.now().isoformat()
    }
    
    reward_logs.append(reward_log)
    
    # Forward to the training visualizer if available
    if training_visualizer is not None and hasattr(training_visualizer, 'record_reward_data'):
        training_visualizer.record_reward_data(step, reward_type, rewards, completions)

# =============================================================================
# Sample-level reward functions
# =============================================================================

def reward_label_consistency_batch(completions, **kwargs):
    """Reward for label consistency between outputs and target labels."""
    global compliance_calculator, current_training_step, current_prompt_attributes
    rewards = []
    print(f"ğŸ¯ Step {current_training_step} - evaluating label consistency reward ({len(completions)} samples)")
    
    # Debug: inspect kwargs content in the first few steps
    if current_training_step <= 2:  # Show detailed info for the first 3 steps
        print(f"ğŸ” Debug kwargs keys: {list(kwargs.keys())}")
        if 'prompts' in kwargs:
            print(f"ğŸ” Prompts available: {len(kwargs['prompts'])}")
            
        # Show full prompt and completion pairs
        print("=" * 80)
        print(f"ğŸ“ STEP {current_training_step} - PROMPT and COMPLETION comparison")
        print("=" * 80)
        
        for i, completion in enumerate(completions[:2]):  # Only show first 2 samples
            print(f"\nğŸ”¸ Sample {i+1}:")
            print(f"ğŸ“¥ å®Œæ•´PROMPT:")
            if 'prompts' in kwargs and i < len(kwargs['prompts']):
                prompt = kwargs['prompts'][i]
                print(f"'{prompt}'")
            else:
                print("  [PROMPTæœªæ‰¾åˆ°]")
            
            print(f"\nğŸ“¤ å®Œæ•´COMPLETION:")
            print(f"'{completion}'")
            
            print(f"\nğŸ” åˆ†ç¦»åçš„ç”Ÿæˆå†…å®¹:")
            try:
                if 'prompts' in kwargs and i < len(kwargs['prompts']):
                    prompt = kwargs['prompts'][i]
                    _, separated_generation = separate_prompt_and_generation_global(completion, prompt)
                else:
                    _, separated_generation = separate_prompt_and_generation_global(completion, "")
                print(f"'{separated_generation[:300]}...'")
            except Exception as e:
                print(f"   âŒ åˆ†ç¦»å¤±è´¥: {e}")
                separated_generation = completion
            
            print("-" * 60)
        print("=" * 80)
        
    # ä»promptä¸­æå–çœŸå®çš„ç›®æ ‡å±æ€§ï¼ˆæ­£ç¡®æ–¹å¼ï¼‰
    if 'prompts' in kwargs:
        update_current_prompt_attributes_from_prompts(kwargs['prompts'])
    else:
        # å¦‚æœæ²¡æœ‰promptsï¼Œå°è¯•æ—§æ–¹å¼ï¼ˆä½†ä¸å¤ªå‡†ç¡®ï¼‰
        if not current_prompt_attributes:
            update_current_prompt_attributes()
    
    target_label = current_prompt_attributes.get('target_label', 'Society & Culture')
    
    for i, completion in enumerate(completions):
        # æ£€æŸ¥compliance_calculatoræ˜¯å¦ä¸ºNoneï¼ˆYahooæ•°æ®é›†æƒ…å†µï¼‰
        if compliance_calculator is None:
            # ä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…ä½œä¸ºå…œåº•
            completion_lower = completion.lower()
            if target_label.lower() in completion_lower:
                score = 1.0
            else:
                # æ£€æŸ¥Yahooæ ‡ç­¾ç›¸å…³è¯æ±‡
                label_words = {
                    'society & culture': ['culture', 'society', 'social', 'community', 'tradition', 'custom'],
                    'science & mathematics': ['science', 'math', 'physics', 'chemistry', 'biology', 'research'],
                    'health': ['health', 'medical', 'doctor', 'medicine', 'treatment', 'symptoms'],
                    'education & reference': ['education', 'school', 'learn', 'study', 'knowledge', 'reference'],
                    'computers & internet': ['computer', 'internet', 'software', 'technology', 'digital', 'online'],
                    'sports': ['sports', 'game', 'team', 'player', 'match', 'competition'],
                    'business & finance': ['business', 'finance', 'money', 'investment', 'market', 'economy'],
                    'entertainment & music': ['entertainment', 'music', 'movie', 'film', 'show', 'performance'],
                    'family & relationships': ['family', 'relationship', 'marriage', 'parent', 'child', 'love'],
                    'politics & government': ['politics', 'government', 'policy', 'election', 'democracy', 'law']
                }
                
                words = label_words.get(target_label.lower(), [])
                matches = sum(1 for word in words if word in completion_lower)
                score = min(matches / max(len(words), 1), 1.0)
        else:
            score = compliance_calculator.calculate_label_consistency(completion, target_label)
        
        # æ­£å‘æ¿€åŠ±ç­–ç•¥ï¼šå®Œå…¨åŒ¹é…ç»™é«˜å¥–åŠ±ï¼Œä¸åŒ¹é…ç»™ä¸­æ€§å¥–åŠ±ï¼ˆé¿å…è´Ÿåˆ†ï¼‰
        if score >= 1.0:  # å®Œå…¨åŒ¹é…
            final_reward = 0.5
        elif score >= 0.5:  # éƒ¨åˆ†åŒ¹é…
            final_reward = 0.2
        else:  # ä¸åŒ¹é…ï¼Œä½†ä¸æƒ©ç½šï¼ˆä¸­æ€§ï¼‰
            final_reward = 0.0
        rewards.append(final_reward)
        
        if i < 2:
            if compliance_calculator is not None:
                extracted_label = compliance_calculator.extract_label_from_json(completion)
            else:
                # Yahooæ•°æ®é›†ï¼šä»completionä¸­æå–æ ‡ç­¾
                try:
                    import json
                    completion_json = json.loads(completion)
                    extracted_label = completion_json.get('output', 'unknown')
                except (json.JSONDecodeError, KeyError, TypeError):
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…
                    completion_lower = completion.lower()
                    if any(word in completion_lower for word in ['culture', 'society', 'social', 'community']):
                        extracted_label = 'Society & Culture'
                    elif any(word in completion_lower for word in ['science', 'math', 'physics', 'chemistry']):
                        extracted_label = 'Science & Mathematics'
                    elif any(word in completion_lower for word in ['health', 'medical', 'doctor', 'medicine']):
                        extracted_label = 'Health'
                    elif any(word in completion_lower for word in ['education', 'school', 'learn', 'study']):
                        extracted_label = 'Education & Reference'
                    elif any(word in completion_lower for word in ['computer', 'internet', 'software', 'technology']):
                        extracted_label = 'Computers & Internet'
                    elif any(word in completion_lower for word in ['sports', 'game', 'team', 'player']):
                        extracted_label = 'Sports'
                    elif any(word in completion_lower for word in ['business', 'finance', 'money', 'investment']):
                        extracted_label = 'Business & Finance'
                    elif any(word in completion_lower for word in ['entertainment', 'music', 'movie', 'film']):
                        extracted_label = 'Entertainment & Music'
                    elif any(word in completion_lower for word in ['family', 'relationship', 'marriage', 'parent']):
                        extracted_label = 'Family & Relationships'
                    elif any(word in completion_lower for word in ['politics', 'government', 'policy', 'election']):
                        extracted_label = 'Politics & Government'
                    else:
                        extracted_label = 'unknown'
            # æ˜¾ç¤ºå¤„ç†åçš„ç”Ÿæˆå†…å®¹è€Œä¸æ˜¯åŸå§‹completion
            generated_text = extract_text_content_global(completion)
            print(f"   æ ·æœ¬{i+1}: ç›®æ ‡æ ‡ç­¾={target_label}, æå–æ ‡ç­¾={extracted_label}, åˆ†æ•°={score:.2f}, å¥–åŠ±={final_reward:.4f}")
            print(f"   ç”Ÿæˆå†…å®¹: {generated_text[:100]}...")
    
    rewards_tensor = torch.tensor(rewards, dtype=torch.float32)
    
    # ä»kwargsä¸­ç§»é™¤stepå‚æ•°ï¼Œé¿å…é‡å¤ä¼ é€’
    kwargs_copy = kwargs.copy()
    kwargs_copy.pop('step', None)
    
    log_reward_details(current_training_step, "sentiment_consistency", completions, rewards_tensor, **kwargs_copy)
    return rewards_tensor

def reward_attribute_compliance_batch(completions, **kwargs):
    """å±æ€§è¦æ±‚ç¬¦åˆåº¦å¥–åŠ±"""
    global compliance_calculator, current_training_step, current_prompt_attributes
    rewards = []
    print(f"ğŸ” Step {current_training_step} - è¯„ä¼°å±æ€§ç¬¦åˆåº¦å¥–åŠ± ({len(completions)}ä¸ªæ ·æœ¬)")
    
    # ç¡®ä¿æœ‰å½“å‰æç¤ºå±æ€§
    if not current_prompt_attributes:
        update_current_prompt_attributes()
    
    for i, completion in enumerate(completions):
        total_score = 0.0
        attribute_count = 0
        
        # Yahooæ•°æ®é›†ä¸éœ€è¦è¯„ä¼°èœç³»å±æ€§
        # è¿™äº›æ˜¯Yelpæ•°æ®é›†ç‰¹æœ‰çš„å±æ€§
        
        # è¯„ä¼°Yahooå±æ€§
        for attr_name in ['question_type', 'user_intent', 'answer_tone', 'complexity_level', 'evidence_expectation', 'style', 'domain_subtopic']:
            if attr_name in current_prompt_attributes:
                attr_score = compliance_calculator.calculate_attribute_keyword_match(
                    completion, 
                    attr_name, 
                    current_prompt_attributes[attr_name],
                    current_prompt_attributes.get('target_label')
                )
                total_score += attr_score * 0.5  # é™ä½å…¶ä»–å±æ€§çš„æƒé‡
                attribute_count += 0.5
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        if attribute_count > 0:
            avg_score = total_score / attribute_count
        else:
            avg_score = 0.5
        
        # æ­£å‘æ¿€åŠ±ç­–ç•¥ï¼šç¬¦åˆå±æ€§ç»™å¥–åŠ±ï¼Œä¸ç¬¦åˆä¸æƒ©ç½š
        if avg_score >= 0.8:  # é«˜åº¦ç¬¦åˆå±æ€§è¦æ±‚
            final_reward = 0.25
        elif avg_score >= 0.6:  # è¾ƒå¥½ç¬¦åˆ
            final_reward = 0.15
        elif avg_score >= 0.4:  # åŸºæœ¬ç¬¦åˆ
            final_reward = 0.1
        elif avg_score >= 0.2:  # éƒ¨åˆ†ç¬¦åˆ
            final_reward = 0.05
        else:  # ä¸ç¬¦åˆï¼Œä½†ä¸æƒ©ç½š
            final_reward = 0.0
        rewards.append(final_reward)
        
        if i < 2:
            target_label = current_prompt_attributes.get('target_label', 'N/A')
            print(f"   æ ·æœ¬{i+1}: ç›®æ ‡æ ‡ç­¾={target_label}, å¹³å‡å±æ€§åˆ†æ•°={avg_score:.2f}, å¥–åŠ±={final_reward:.4f}")
    
    rewards_tensor = torch.tensor(rewards, dtype=torch.float32)
    
    # ä»kwargsä¸­ç§»é™¤stepå‚æ•°ï¼Œé¿å…é‡å¤ä¼ é€’
    kwargs_copy = kwargs.copy()
    kwargs_copy.pop('step', None)
    
    log_reward_details(current_training_step, "attribute_compliance", completions, rewards_tensor, **kwargs_copy)
    return rewards_tensor

def reward_length_compliance_batch(completions, **kwargs):
    """é•¿åº¦è¦æ±‚ç¬¦åˆåº¦å¥–åŠ±"""
    global compliance_calculator, current_training_step, current_prompt_attributes
    rewards = []
    print(f"ğŸ“ Step {current_training_step} - è¯„ä¼°é•¿åº¦ç¬¦åˆåº¦å¥–åŠ± ({len(completions)}ä¸ªæ ·æœ¬)")
    
    # ç¡®ä¿æœ‰å½“å‰æç¤ºå±æ€§
    if not current_prompt_attributes:
        update_current_prompt_attributes()
    
    target_length = current_prompt_attributes.get('length', 200)
    
    for i, completion in enumerate(completions):
        score = compliance_calculator.calculate_length_compliance(completion, target_length, tolerance=25)
        
        # æ­£å‘æ¿€åŠ±ç­–ç•¥ï¼šåœ¨èŒƒå›´å†…ç»™å¥–åŠ±ï¼Œè¶…å‡ºèŒƒå›´ä¸æƒ©ç½š
        if score >= 1.0:  # å®Œå…¨ç¬¦åˆé•¿åº¦è¦æ±‚
            final_reward = 0.2
        elif score >= 0.8:  # åŸºæœ¬ç¬¦åˆ
            final_reward = 0.1
        elif score >= 0.5:  # å¯æ¥å—èŒƒå›´
            final_reward = 0.05
        else:  # ä¸ç¬¦åˆï¼Œä½†ä¸æƒ©ç½šï¼ˆé¿å…ä¸å…¶ä»–è¦æ±‚å†²çªï¼‰
            final_reward = 0.0
        rewards.append(final_reward)
        
        if i < 2:
            # è®¡ç®—å®é™…é•¿åº¦ç”¨äºæ˜¾ç¤º
            text_content = extract_text_content_global(completion)
            actual_length = len(text_content.split())
            
            # æ˜¾ç¤ºç›®æ ‡é•¿åº¦ä¿¡æ¯
            if isinstance(target_length, dict):
                length_info = f"ç›®æ ‡èŒƒå›´={target_length['min']}-{target_length['max']}"
            else:
                length_info = f"ç›®æ ‡é•¿åº¦={target_length}"
            
            print(f"   æ ·æœ¬{i+1}: {length_info}, å®é™…é•¿åº¦={actual_length}, åˆ†æ•°={score:.2f}, å¥–åŠ±={final_reward:.4f}")
    
    rewards_tensor = torch.tensor(rewards, dtype=torch.float32)
    
    # ä»kwargsä¸­ç§»é™¤stepå‚æ•°ï¼Œé¿å…é‡å¤ä¼ é€’
    kwargs_copy = kwargs.copy()
    kwargs_copy.pop('step', None)
    
    log_reward_details(current_training_step, "length_compliance", completions, rewards_tensor, **kwargs_copy)
    return rewards_tensor

# =============================================================================
# ç”Ÿæˆè´¨é‡å¥–åŠ±å‡½æ•°
# =============================================================================

def extract_clean_review_global(generated_text, prompt=""):
    """ä»ç”Ÿæˆæ–‡æœ¬ä¸­æå–å¹²å‡€çš„è¯„è®ºå†…å®¹ï¼ˆä¿æŒJSONæ ¼å¼ï¼‰"""
    import re
    import json
    
    # ç§»é™¤promptéƒ¨åˆ†
    if prompt and generated_text.startswith(prompt):
        content = generated_text[len(prompt):].strip()
    else:
        content = generated_text.strip()
    
    # å°è¯•æå–å®Œæ•´çš„JSON
    json_pattern = r'\{[^{}]*"input"\s*:\s*"[^"]*"[^{}]*"output"\s*:\s*"[^"]*"[^{}]*\}'
    json_match = re.search(json_pattern, content)
    if json_match:
        try:
            # éªŒè¯JSONæ ¼å¼æ˜¯å¦æ­£ç¡®
            json_str = json_match.group(0)
            parsed_json = json.loads(json_str)
            if 'input' in parsed_json and 'output' in parsed_json:
                return json_str  # è¿”å›å®Œæ•´çš„JSON
        except json.JSONDecodeError:
            pass
    
    # å¦‚æœæ²¡æ‰¾åˆ°å®Œæ•´JSONï¼Œå°è¯•æå–inputå­—æ®µçš„å†…å®¹
    input_match = re.search(r'"input"\s*:\s*"([^"]*)"', content)
    if input_match:
        input_content = input_match.group(1)
        # å°è¯•æå–outputå­—æ®µ
        output_match = re.search(r'"output"\s*:\s*"([^"]*)"', content)
        if output_match:
            output_content = output_match.group(1)
            # é‡æ„JSON
            return f'{{"input": "{input_content}", "output": "{output_content}"}}'
        else:
            # åªæœ‰inputï¼Œæ·»åŠ é»˜è®¤output
            return f'{{"input": "{input_content}", "output": "unknown"}}'
    
    # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•æå–å¼•å·å†…çš„é•¿å†…å®¹
    quote_matches = re.findall(r'"([^"]{20,})"', content)
    if quote_matches:
        # é€‰æ‹©æœ€é•¿çš„åŒ¹é…ä½œä¸ºä¸»è¦å†…å®¹ï¼ŒåŒ…è£…æˆJSON
        longest_content = max(quote_matches, key=len)
        return f'{{"input": "Text: {longest_content}", "output": "unknown"}}'
    
    # æœ€åçš„å…œåº•ï¼šç§»é™¤æŒ‡ä»¤æ€§æ–‡æœ¬ï¼ŒåŒ…è£…æˆJSON
    clean_patterns = [
        r'Here is an example.*?:',
        r'Here\'s an example.*?:',
        r'Do NOT.*?\.',
        r'Note that.*?\.',
        r'```.*?```',
        r'""".*?"""',
        r'Example.*?:',
    ]
    
    for pattern in clean_patterns:
        content = re.sub(pattern, '', content, flags=re.DOTALL | re.IGNORECASE)
    
    # æ¸…ç†å¤šä½™ç©ºç™½å’Œç‰¹æ®Šå­—ç¬¦
    content = re.sub(r'\s+', ' ', content).strip()
    content = re.sub(r'^["\'\s]+|["\'\s]+$', '', content)
    
    if len(content) > 10:
        return f'{{"input": "Text: {content}", "output": "unknown"}}'
    else:
        return '{"input": "Text: Invalid generation", "output": "unknown"}'

def calculate_generation_quality_score_global(generated_text, prompt=""):
    """ä¸¥æ ¼ç‰ˆç”Ÿæˆè´¨é‡è¯„åˆ†(ä¿æŒåŸæ¥ä¸¥è‹›é€»è¾‘)ä½†å°†æƒ…æ„Ÿæ ‡ç­¾æ›¿æ¢ä¸ºYahooä¸»é¢˜æ ‡ç­¾ã€‚

    åŸå§‹é€»è¾‘ç‰¹ç‚¹ï¼š
    - ä½åŸºç¡€åˆ†(0.1)éœ€è¦é€šè¿‡ç»“æ„/æ ¼å¼æ¥èµšå–åˆ†æ•°
    - å¯¹ä»£ç /æ¨¡æ¿æ®‹ç•™ã€é‡å¤ã€æŒ‡ä»¤æ€§æ ·å¼æ–½åŠ è¾ƒå¼ºæƒ©ç½š
    - JSONæ ¼å¼ä¸å­—æ®µè§„èŒƒæä¾›ä¸»è¦æ­£å‘åŠ åˆ†

    ä¿®æ”¹ç‚¹ï¼šä»…æ›¿æ¢ valid_sentiments -> YAHOO_LABELSï¼Œä¸è°ƒæ•´å…¶ä½™æƒ©ç½š/å¥–åŠ±å¹…åº¦ã€‚
    """
    import json
    import re

    clean_content = extract_clean_review_global(generated_text, prompt)

    # èµ·å§‹åŸºç¡€åˆ†æ•°
    quality_score = 0.1

    # ä¸è‰¯æ¨¡å¼å¼ºæƒ©ç½š(ä¿æŒåŸå¹…åº¦)
    bad_patterns = [
        'import ', 'def ', 'class ', 'function', '```', 'python', 'code', 'script',
        '# ', 'return ', 'print(', 'if __name__', 'from ', 'pipeline', 'random.',
        'gen_', 'tokenizer', 'model_output', 'transformers', 'torch', 'numpy',
        'def synthesize', 'def gen_review', 'def get_random'
    ]
    bad_count = sum(1 for pattern in bad_patterns if pattern.lower() in generated_text.lower())
    if bad_count > 0:
        quality_score -= min(0.8, bad_count * 0.3)

    # é‡å¤å­—ç¬¦å¼ºæƒ©ç½š
    if re.findall(r'(.)\1{10,}', generated_text):
        quality_score -= 0.6

    # æŒ‡ä»¤æ€§æ–‡æœ¬æƒ©ç½š
    instruction_phrases = [
        'here is an example', 'here\'s an example', 'do not add', 'note that you', 
        'template', 'format', 'please see', 'answer:', 'step 1:', 'step 2:',
        'feel free', 'let me know', 'best regards'
    ]
    bad_instructions = sum(1 for phrase in instruction_phrases if phrase.lower() in generated_text.lower())
    if bad_instructions > 0:
        quality_score -= min(0.6, bad_instructions * 0.2)

    # âœ… JSONæ ¼å¼å¥–åŠ±ä¸å­—æ®µæ ¡éªŒ
    try:
        parsed_json = json.loads(clean_content)
        if 'input' in parsed_json and 'output' in parsed_json:
            quality_score += 0.4
            input_content = parsed_json.get('input', '')
            if input_content.startswith('Text: ') and len(input_content) > 20:
                quality_score += 0.3
                review_text = input_content[6:]
                word_count = len(review_text.split())
                if 20 <= word_count <= 300:
                    quality_score += 0.2
                elif word_count < 10:
                    quality_score -= 0.3
                elif word_count > 500:
                    quality_score -= 0.1
            else:
                quality_score -= 0.2

            # å°†åŸæ¥çš„æƒ…æ„Ÿæ ‡ç­¾æ›¿æ¢ä¸ºYahooæ ‡ç­¾é›†åˆ
            output_content = parsed_json.get('output', '')
            YAHOO_LABELS = [
                'Society & Culture','Science & Mathematics','Health','Education & Reference',
                'Computers & Internet','Sports','Business & Finance','Entertainment & Music',
                'Family & Relationships','Politics & Government'
            ]
            if output_content in YAHOO_LABELS:
                quality_score += 0.1
            elif output_content == 'unknown':
                quality_score -= 0.1
            else:
                quality_score -= 0.2
        else:
            quality_score -= 0.3
    except json.JSONDecodeError:
        quality_score -= 0.5

    # å™ªéŸ³æ ¼å¼æ‰£åˆ†
    noise_patterns = [r'```', r'"""', r'\{[^}]*\}[^}]*\{', r'\\n\\n\\n+']
    for pattern in noise_patterns:
        if re.search(pattern, generated_text):
            quality_score -= 0.1

    # ä»£ç æ®‹ç•™æ‰£åˆ†
    code_indicators = ['def ', 'import ', 'class ', '# ', '// ', '/* ']
    if any(indicator in generated_text for indicator in code_indicators):
        quality_score -= 0.3

    return max(0.0, min(1.0, quality_score))

def reward_generation_quality_batch(completions, **kwargs):
    """ç”Ÿæˆè´¨é‡å¥–åŠ±å‡½æ•° (ä¸¥æ ¼ç‰ˆ + Yahooæ ‡ç­¾)"""
    global current_training_step
    rewards = []
    prompts = kwargs.get('prompts', [''] * len(completions))

    print(f"ğŸ¨ Step {current_training_step} - è¯„ä¼°ç”Ÿæˆè´¨é‡å¥–åŠ± ({len(completions)}ä¸ªæ ·æœ¬)")

    for i, completion in enumerate(completions):
        prompt = prompts[i] if i < len(prompts) else ""
        quality_score = calculate_generation_quality_score_global(completion, prompt)

        # ä¿æŒåŸå…ˆåˆ†æ®µé˜ˆå€¼ä¸å¥–åŠ±æ˜ å°„ï¼ˆä¸¥æ ¼ï¼Œä¸çº¿æ€§ï¼‰
        if quality_score >= 0.9:
            reward = 0.3
        elif quality_score >= 0.7:
            reward = 0.2
        elif quality_score >= 0.5:
            reward = 0.1
        elif quality_score >= 0.3:
            reward = 0.05
        else:
            reward = 0.0
        rewards.append(reward)

        if i < 2:
            clean_content = extract_clean_review_global(completion, prompt)
            print(f"   æ ·æœ¬{i+1}: è´¨é‡åˆ†æ•°={quality_score:.2f}, å¥–åŠ±={reward:.4f}")
            print(f"   æ¸…ç†åå†…å®¹: {clean_content[:100]}...")

    rewards_tensor = torch.tensor(rewards, dtype=torch.float32)
    kwargs_copy = kwargs.copy(); kwargs_copy.pop('step', None)
    log_reward_details(current_training_step, "generation_quality", completions, rewards_tensor, **kwargs_copy)
    return rewards_tensor

# =============================================================================
# Batchçº§åˆ«å¥–åŠ±å‡½æ•°
# =============================================================================

def reward_yelp_semantic_diversity_batch(completions, **kwargs):
    """é¤å…è¯„è®ºè¯­ä¹‰å¤šæ ·æ€§å¥–åŠ±ï¼ˆç»“åˆNovelSumæ€æƒ³å’ŒYelpæ•°æ®ç‰¹è‰²ï¼Œæ”¯æŒåŠ¨æ€å‚è€ƒæ•°æ®ï¼‰"""
    global novelsum_calculator, current_training_step
    
    print(f"ğŸ½ï¸ Step {current_training_step} - è¯„ä¼°é¤å…è¯„è®ºè¯­ä¹‰å¤šæ ·æ€§å¥–åŠ± (batch size: {len(completions)})")
    
    try:
        # ä»completionsä¸­æå–å®é™…æ–‡æœ¬å†…å®¹
        texts = [extract_text_content_global(completion) for completion in completions]
        texts = [text for text in texts if text and len(text) > 10]
        
        if len(texts) < 2:
            print("   âš ï¸ æœ‰æ•ˆæ–‡æœ¬å°‘äº2ä¸ªï¼Œè¿”å›åŸºç¡€å¥–åŠ±")
            return torch.zeros(len(completions), dtype=torch.float32)
        
        # ä½¿ç”¨NovelSumè®¡ç®—å¤šæ ·æ€§
        novelsum_score = novelsum_calculator.calculate_novelsum_score(
            texts,
            density_power=0.5,
            distance_power=1.0,
            neighbors=min(10, len(texts))
        )
        
        # è®¡ç®—é¤å…è¯„è®ºç‰¹è‰²å¤šæ ·æ€§
        print(f"ğŸ” DEBUG: å¼€å§‹è®¡ç®—é¤å…å¤šæ ·æ€§...")
        restaurant_diversity = calculate_restaurant_specific_diversity(texts)
        print(f"ğŸ” DEBUG: é¤å…å¤šæ ·æ€§è®¡ç®—å®Œæˆ: {restaurant_diversity} (ç±»å‹: {type(restaurant_diversity)})")
        
        # ç»“åˆä¸¤ç§å¤šæ ·æ€§åº¦é‡
        print(f"ğŸ” DEBUG: å¼€å§‹ç»“åˆå¤šæ ·æ€§åº¦é‡...")
        print(f"ğŸ” DEBUG: novelsum_score = {novelsum_score} (ç±»å‹: {type(novelsum_score)})")
        print(f"ğŸ” DEBUG: restaurant_diversity = {restaurant_diversity} (ç±»å‹: {type(restaurant_diversity)})")
        
        combined_diversity = (novelsum_score * 0.6 + restaurant_diversity * 0.4)
        print(f"ğŸ” DEBUG: åˆå§‹combined_diversity = {combined_diversity} (ç±»å‹: {type(combined_diversity)})")
        
        combined_diversity = float(combined_diversity)  # ç¡®ä¿æ˜¯æ ‡é‡
        print(f"ğŸ” DEBUG: è½¬æ¢åcombined_diversity = {combined_diversity} (ç±»å‹: {type(combined_diversity)})")
        
        # å°†batchçº§åˆ«çš„å¥–åŠ±åˆ†é…ç»™æ¯ä¸ªæ ·æœ¬
        print(f"ğŸ” DEBUG: å¼€å§‹è®¡ç®—batchå¥–åŠ±...")
        batch_reward = (combined_diversity - 0.5) * 0.6  # æ ‡å‡†åŒ–åˆ°[-0.3, 0.3]
        print(f"ğŸ” DEBUG: åˆå§‹batch_reward = {batch_reward} (ç±»å‹: {type(batch_reward)})")
        
        batch_reward = float(batch_reward)  # ç¡®ä¿æ˜¯æ ‡é‡
        print(f"ğŸ” DEBUG: è½¬æ¢åbatch_reward = {batch_reward} (ç±»å‹: {type(batch_reward)})")
        
        rewards = torch.full((len(completions),), batch_reward, dtype=torch.float32)
        print(f"ğŸ” DEBUG: rewardså¼ é‡åˆ›å»ºå®Œæˆ: shape={rewards.shape}")
        
        # åŠ¨æ€æ›´æ–°å‚è€ƒæ•°æ®æ± ï¼ˆæ›´å®½æ¾çš„æ¡ä»¶ä»¥ä¿ƒè¿›å­¦ä¹ ï¼‰
        print(f"ğŸ” DEBUG: æ£€æŸ¥æ˜¯å¦æ›´æ–°å‚è€ƒæ± ...")
        print(f"ğŸ” DEBUG: hasattr(novelsum_calculator, 'add_training_samples') = {hasattr(novelsum_calculator, 'add_training_samples')}")
        print(f"ğŸ” DEBUG: batch_reward = {batch_reward}, batch_reward > -0.1 = {batch_reward > -0.1}")
        
        if hasattr(novelsum_calculator, 'add_training_samples') and batch_reward > -0.1:
            try:
                # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„è´¨é‡åˆ†æ•°ï¼ˆåŸºäºå¤šæ ·æ€§å’Œé•¿åº¦ï¼‰
                quality_scores = []
                for text in texts:
                    length_score = min(1.0, len(text.split()) / 150.0)  # é•¿åº¦å½’ä¸€åŒ–
                    
                    # å®‰å…¨åœ°è®¡ç®—individual_novelty
                    if hasattr(novelsum_calculator, 'reference_manager'):
                        individual_novelty = novelsum_calculator.reference_manager.compute_novelty_score(text)
                        # ç¡®ä¿æ˜¯æ ‡é‡
                        if isinstance(individual_novelty, torch.Tensor):
                            individual_novelty = float(individual_novelty.item())
                        else:
                            individual_novelty = float(individual_novelty)
                    else:
                        individual_novelty = 0.5
                    
                    quality = (individual_novelty * 0.7 + length_score * 0.3)
                    quality_scores.append(float(quality))
                
                # æ·»åŠ é«˜è´¨é‡æ ·æœ¬åˆ°å‚è€ƒæ± 
                added_count = novelsum_calculator.add_training_samples(texts, quality_scores)
                if added_count > 0:
                    print(f"   ğŸ“Š æ·»åŠ {added_count}ä¸ªé«˜è´¨é‡æ ·æœ¬åˆ°åŠ¨æ€å‚è€ƒæ± ")
                    
            except Exception as e:
                print(f"   âš ï¸ åŠ¨æ€å‚è€ƒæ± æ›´æ–°å¤±è´¥: {e}")
                import traceback
                print(f"   è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        
        print(f"   NovelSumåˆ†æ•°: {novelsum_score:.3f}, é¤å…å¤šæ ·æ€§: {restaurant_diversity:.3f}")
        print(f"   ç»¼åˆå¤šæ ·æ€§: {combined_diversity:.3f}, batchå¥–åŠ±: {batch_reward:.4f}")
        
        # å®šæœŸæ‰“å°å‚è€ƒæ± ç»Ÿè®¡ä¿¡æ¯
        if current_training_step % 20 == 0 and hasattr(novelsum_calculator, 'get_reference_statistics'):
            stats = novelsum_calculator.get_reference_statistics()
            print(f"   ğŸ“ˆ å‚è€ƒæ± çŠ¶æ€: {stats.get('total_count', 0)}æ ·æœ¬ (åŸå§‹:{stats.get('original_count', 0)}, åŠ¨æ€:{stats.get('dynamic_count', 0)})")
        
        # ä»kwargsä¸­ç§»é™¤stepå‚æ•°ï¼Œé¿å…é‡å¤ä¼ é€’
        kwargs_copy = kwargs.copy()
        kwargs_copy.pop('step', None)
        log_reward_details(current_training_step, "yelp_semantic_diversity", completions, rewards, **kwargs_copy)
        return rewards
        
    except Exception as e:
        print(f"   âš ï¸ å¤šæ ·æ€§è®¡ç®—å¤±è´¥: {e}")
        return torch.zeros(len(completions), dtype=torch.float32)

def reward_inter_sample_diversity_batch(completions, **kwargs):
    """æ‰¹æ¬¡å†…æ ·æœ¬å¤šæ ·æ€§å¥–åŠ±"""
    global current_training_step
    
    print(f"ğŸ”„ Step {current_training_step} - è¯„ä¼°æ‰¹æ¬¡å†…å¤šæ ·æ€§å¥–åŠ± (batch size: {len(completions)})")
    
    try:
        texts = [extract_text_content_global(completion) for completion in completions]
        texts = [text for text in texts if text and len(text) > 10]
        
        if len(texts) < 2:
            return torch.zeros(len(completions), dtype=torch.float32)
        
        # ä½¿ç”¨ç®€å•çš„jaccardè·ç¦»è®¡ç®—å¤šæ ·æ€§
        def jaccard_similarity(text1, text2):
            words1 = set(text1.lower().split())
            words2 = set(text2.lower().split())
            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))
            return intersection / union if union > 0 else 0.0
        
        # è®¡ç®—æ‰€æœ‰æ ·æœ¬å¯¹çš„å¤šæ ·æ€§
        diversities = []
        for i in range(len(texts)):
            for j in range(i + 1, len(texts)):
                similarity = jaccard_similarity(texts[i], texts[j])
                diversity = 1.0 - similarity
                diversities.append(diversity)
        
        if not diversities:
            return torch.zeros(len(completions), dtype=torch.float32)
        
        avg_diversity = np.mean(diversities)
        avg_diversity = float(avg_diversity)  # ç¡®ä¿æ˜¯æ ‡é‡
        
        # æ ‡å‡†åŒ–å¥–åŠ±åˆ°[-0.2, 0.2]èŒƒå›´
        batch_reward = (avg_diversity - 0.5) * 0.4
        batch_reward = float(batch_reward)  # ç¡®ä¿æ˜¯æ ‡é‡
        rewards = torch.full((len(completions),), batch_reward, dtype=torch.float32)
        
        print(f"   å¹³å‡Jaccardå¤šæ ·æ€§: {avg_diversity:.3f}, batchå¥–åŠ±: {batch_reward:.4f}")
        
        # ä»kwargsä¸­ç§»é™¤stepå‚æ•°ï¼Œé¿å…é‡å¤ä¼ é€’
        kwargs_copy = kwargs.copy()
        kwargs_copy.pop('step', None)
        log_reward_details(current_training_step, "inter_sample_diversity", completions, rewards, **kwargs_copy)
        return rewards
        
    except Exception as e:
        print(f"   âš ï¸ æ‰¹æ¬¡å¤šæ ·æ€§è®¡ç®—å¤±è´¥: {e}")
        return torch.zeros(len(completions), dtype=torch.float32)

# =============================================================================
# ç»¼åˆå¥–åŠ±å‡½æ•°
# =============================================================================

def create_weighted_reward_functions():
    """åˆ›å»ºåŠ æƒçš„å¥–åŠ±å‡½æ•° - åŒ…å«Sampleçº§åˆ«å’ŒBatchçº§åˆ«çš„å¥–åŠ±"""
    
    def weighted_sentiment_consistency(completions, **kwargs):
        """åŠ æƒæƒ…æ„Ÿä¸€è‡´æ€§å¥–åŠ±å‡½æ•°"""
        global reward_call_counter, current_training_step
        
        # æ›´æ–°å…¨å±€è®¡æ•°å™¨å’Œè®­ç»ƒæ­¥æ•°
        reward_call_counter += 1
        current_training_step = reward_call_counter // 5  # æ¯5æ¬¡å¥–åŠ±å‡½æ•°è°ƒç”¨ä¸ºä¸€ä¸ªè®­ç»ƒæ­¥ï¼ˆç°åœ¨æœ‰5ä¸ªå¥–åŠ±å‡½æ•°ï¼‰
        
        # æ›´æ–°å½“å‰æ‰¹æ¬¡çš„æç¤ºå±æ€§
        update_current_prompt_attributes(current_training_step)
        
        base_rewards = reward_sentiment_consistency_batch(completions, **kwargs)
        return base_rewards  # è¿”å›æœªåŠ æƒçš„åŸºç¡€å¥–åŠ±
    
    def weighted_attribute_compliance(completions, **kwargs):
        """å±æ€§ç¬¦åˆåº¦å¥–åŠ±å‡½æ•°ï¼ˆè¿”å›æœªåŠ æƒå€¼ï¼‰"""
        base_rewards = reward_attribute_compliance_batch(completions, **kwargs)
        return base_rewards  # è¿”å›æœªåŠ æƒçš„åŸºç¡€å¥–åŠ±
    
    def weighted_length_compliance(completions, **kwargs):
        """é•¿åº¦ç¬¦åˆåº¦å¥–åŠ±å‡½æ•°ï¼ˆè¿”å›æœªåŠ æƒå€¼ï¼‰"""
        base_rewards = reward_length_compliance_batch(completions, **kwargs)
        return base_rewards  # è¿”å›æœªåŠ æƒçš„åŸºç¡€å¥–åŠ±
    
    def weighted_generation_quality(completions, **kwargs):
        """ç”Ÿæˆè´¨é‡å¥–åŠ±å‡½æ•°"""
        base_rewards = reward_generation_quality_batch(completions, **kwargs)
        return base_rewards  # è¿”å›æœªåŠ æƒçš„åŸºç¡€å¥–åŠ±
    
    def weighted_batch_diversity(completions, **kwargs):
        """Batchçº§åˆ«å¤šæ ·æ€§å¥–åŠ±å‡½æ•°"""
        if reward_batch_diversity is None:
            print("âš ï¸ Batchå¤šæ ·æ€§å¥–åŠ±å‡½æ•°ä¸å¯ç”¨ï¼Œè¿”å›ä¸­æ€§å¥–åŠ±")
            return [0.0] * len(completions)
        
        base_rewards = reward_batch_diversity(completions, **kwargs)
        return base_rewards  # è¿”å›æœªåŠ æƒçš„åŸºç¡€å¥–åŠ±
    
    # è®¾ç½®å‡½æ•°å
    weighted_sentiment_consistency.__name__ = "reward_sentiment_consistency"
    weighted_attribute_compliance.__name__ = "reward_attribute_compliance"
    weighted_length_compliance.__name__ = "reward_length_compliance"
    weighted_generation_quality.__name__ = "reward_generation_quality"
    weighted_batch_diversity.__name__ = "reward_batch_diversity"
    
    return [
        weighted_sentiment_consistency,    # Sampleçº§åˆ«: æƒ…æ„Ÿæ ‡ç­¾ä¸€è‡´æ€§
        weighted_attribute_compliance,     # Sampleçº§åˆ«: å±æ€§è¦æ±‚ç¬¦åˆåº¦  
        weighted_length_compliance,        # Sampleçº§åˆ«: é•¿åº¦è¦æ±‚ç¬¦åˆåº¦
        weighted_generation_quality,       # Sampleçº§åˆ«: ç”Ÿæˆè´¨é‡æ§åˆ¶
        weighted_batch_diversity,          # Batchçº§åˆ«: åŸºäºå±€éƒ¨å¯†åº¦çš„å¤šæ ·æ€§å¥–åŠ±
    ]

def save_training_logs(output_dir):
    """ä¿å­˜è®­ç»ƒæ—¥å¿—"""
    global reward_logs
    
    if reward_logs:
        logs_file = f"{output_dir}/reward_logs.json"
        import json
        with open(logs_file, 'w', encoding='utf-8') as f:
            json.dump(reward_logs, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“Š å¥–åŠ±æ—¥å¿—å·²ä¿å­˜: {logs_file}")

def initialize_reward_globals(td_global, bs_global, rc, nc, al, cc, optimized_sample_config=None, optimized_batch_config=None, embedding_model_path=None):
    """åˆå§‹åŒ–å¥–åŠ±å‡½æ•°æ¨¡å—çš„å…¨å±€å˜é‡ï¼ˆSample + Batchçº§åˆ«ï¼‰"""
    global training_data_global, batch_size_global, reward_calculator, novelsum_calculator, attr_loader, compliance_calculator
    global CURRENT_SAMPLE_REWARDS_CONFIG, CURRENT_BATCH_REWARDS_CONFIG
    
    training_data_global = td_global
    batch_size_global = bs_global
    reward_calculator = rc
    novelsum_calculator = nc  # ä¿ç•™å…¼å®¹æ€§ï¼Œä½†ä¸ä½¿ç”¨
    attr_loader = al
    compliance_calculator = cc
    
    # æ›´æ–°é…ç½®ï¼ˆå¦‚æœæä¾›äº†ä¼˜åŒ–é…ç½®ï¼‰
    if optimized_sample_config:
        CURRENT_SAMPLE_REWARDS_CONFIG.update(optimized_sample_config)
        print(f"âœ… ä½¿ç”¨ä¼˜åŒ–çš„Sampleå¥–åŠ±é…ç½®: {CURRENT_SAMPLE_REWARDS_CONFIG}")
    
    # æ›´æ–°batché…ç½®
    if optimized_batch_config:
        CURRENT_BATCH_REWARDS_CONFIG.update(optimized_batch_config)
        print(f"âœ… ä½¿ç”¨ä¼˜åŒ–çš„Batchå¥–åŠ±é…ç½®: {CURRENT_BATCH_REWARDS_CONFIG}")
    
    # åˆå§‹åŒ–batchå¤šæ ·æ€§è®¡ç®—å™¨
    if embedding_model_path and initialize_batch_diversity_calculator:
        try:
            initialize_batch_diversity_calculator(embedding_model_path, device='cuda', k_penalty=2.0)
            print("âœ… Batchå¤šæ ·æ€§è®¡ç®—å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ Batchå¤šæ ·æ€§è®¡ç®—å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    print("âœ… å¥–åŠ±å‡½æ•°æ¨¡å—å…¨å±€å˜é‡åˆå§‹åŒ–å®Œæˆï¼ˆSample + Batchçº§åˆ«ï¼‰")

def set_training_visualizer(visualizer):
    """è®¾ç½®è®­ç»ƒå¯è§†åŒ–å™¨"""
    global training_visualizer
    training_visualizer = visualizer
    print(f"âœ… è®­ç»ƒå¯è§†åŒ–å™¨å·²è®¾ç½®")