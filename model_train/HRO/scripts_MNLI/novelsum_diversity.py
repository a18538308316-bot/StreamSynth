#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NovelSumå¤šæ ·æ€§è®¡ç®—æ¨¡å— - æ”¯æŒåŠ¨æ€å‚è€ƒæ•°æ®ç®¡ç†
"""
import numpy as np
import torch
import faiss
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import re
from typing import List, Optional
from .dynamic_reference_manager import DynamicReferenceManager

# NovelSumé…ç½®
NOVELSUM_CONFIG = {
    'density_power': 0.5,
    'distance_power': 1.0,
    'neighbors': 10,
    'embedding_model_path': '/public/home/huzhenlin2023/synthetic_data/all-MiniLM-L6-v2',
    'max_length': 512,
    # åŠ¨æ€å‚è€ƒæ•°æ®é…ç½®
    'max_reference_size': 300,
    'min_novelty_threshold': 0.1,  # é™ä½é˜ˆå€¼ä»¥ä¿ƒè¿›åŠ¨æ€æ ·æœ¬æ·»åŠ 
    'original_data_path': '/public/home/huzhenlin2023/paper_2_LLM_Synthesis/synthesis_model_train/original_yelp_train_100_dataset.json',
}

class NovelSumDiversityCalculator:
    """NovelSumå¤šæ ·æ€§è®¡ç®—å™¨ - æ”¯æŒåŠ¨æ€å‚è€ƒæ•°æ®ç®¡ç†"""
    
    def __init__(self, embedding_model_path, device='cuda', max_length=256, 
                 use_dynamic_reference=True, original_data_path=None):
        self.device = device
        self.max_length = max_length
        self.use_dynamic_reference = use_dynamic_reference
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹
        self.embedding_model = SentenceTransformer(embedding_model_path)
        self.embedding_model.to(device)
        
        # åˆå§‹åŒ–å‚è€ƒæ•°æ®ç®¡ç†å™¨
        if use_dynamic_reference:
            self.reference_manager = DynamicReferenceManager(
                embedding_model=self.embedding_model,
                device=device,
                max_total_size=NOVELSUM_CONFIG.get('max_reference_size', 300),
                min_novelty_threshold=NOVELSUM_CONFIG.get('min_novelty_threshold', 0.3),
                original_data_path=original_data_path or NOVELSUM_CONFIG.get('original_data_path')
            )
            print("âœ… åŠ¨æ€å‚è€ƒæ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        else:
            # å‘åå…¼å®¹ï¼šä½¿ç”¨é™æ€å‚è€ƒæ•°æ®
            self.reference_embeddings = None
            self.reference_index = None
            self.setup_static_reference_data()
        
    def get_reference_data(self):
        """è·å–å‚è€ƒæ•°æ®"""
        if self.use_dynamic_reference:
            return self.reference_manager.get_reference_data()
        else:
            return None, self.reference_embeddings, self.reference_index
    
    def add_training_samples(self, texts: List[str], quality_scores: Optional[List[float]] = None):
        """æ·»åŠ è®­ç»ƒæ ·æœ¬åˆ°åŠ¨æ€å‚è€ƒæ± """
        if not self.use_dynamic_reference:
            return 0
        
        return self.reference_manager.batch_add_samples(texts, quality_scores)
    
    def get_reference_statistics(self):
        """è·å–å‚è€ƒæ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        if self.use_dynamic_reference:
            return self.reference_manager.get_statistics()
        else:
            total_count = self.reference_embeddings.shape[0] if self.reference_embeddings is not None else 0
            return {
                'total_count': total_count,
                'mode': 'static',
                'source': 'template'
            }
        
    def setup_static_reference_data(self):
        """è®¾ç½®é™æ€å‚è€ƒæ•°æ®ï¼ˆå‘åå…¼å®¹ï¼‰"""
        # é¤å…è¯„è®ºå‚è€ƒè¯­æ–™ï¼ˆå¯ä»¥æ›¿æ¢ä¸ºå®é™…çš„å‚è€ƒæ•°æ®é›†ï¼‰
        reference_texts = [
            "The food was absolutely delicious and the service was excellent.",
            "Terrible experience, food was cold and service was slow.",
            "Average restaurant with decent food but nothing special.",
            "Amazing atmosphere and great value for money.",
            "Overpriced food with mediocre quality.",
            "Best Italian restaurant in town, highly recommended.",
            "The ambiance was nice but the food was disappointing.",
            "Quick service and tasty food, perfect for lunch.",
            "Romantic setting with exceptional cuisine.",
            "Friendly staff and reasonable prices.",
            "Authentic cuisine with fresh ingredients.",
            "Poor quality food and unfriendly service.",
            "Cozy atmosphere with excellent desserts.",
            "Innovative menu with creative presentations.",
            "Traditional dishes prepared perfectly.",
            "Busy restaurant with good portion sizes.",
            "Elegant dining experience worth the price.",
            "Family-friendly place with kid's menu.",
            "Spicy food with authentic flavors.",
            "Clean restaurant with professional service.",
            "Unique dishes not found elsewhere.",
            "Comfortable seating and pleasant music.",
            "Fresh seafood and great wine selection.",
            "Vegetarian options and healthy choices.",
            "Late night dining with good atmosphere."
        ]
        
        print("ğŸ”„ è®¾ç½®NovelSumé™æ€å‚è€ƒæ•°æ®...")
        try:
            self.reference_embeddings = self.embedding_model.encode(
                reference_texts, 
                convert_to_tensor=True,
                device=self.device
            )
            self.setup_faiss_index_static()
            print("âœ… NovelSumé™æ€å‚è€ƒæ•°æ®è®¾ç½®å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ NovelSumé™æ€å‚è€ƒæ•°æ®è®¾ç½®å¤±è´¥: {e}")
            self.reference_embeddings = None
    
    def setup_faiss_index_static(self):
        """è®¾ç½®FAISSç´¢å¼•ç”¨äºå¿«é€Ÿç›¸ä¼¼åº¦æœç´¢ï¼ˆé™æ€ç‰ˆæœ¬ï¼‰"""
        if self.reference_embeddings is None:
            return
        
        try:
            embedding_dim = self.reference_embeddings.shape[1]
            self.reference_index = faiss.IndexFlatIP(embedding_dim)  # Inner Product (cosine similarity)
            
            # å½’ä¸€åŒ–embeddingsä»¥ä¾¿ä½¿ç”¨å†…ç§¯è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            ref_embeddings_normalized = self.reference_embeddings.cpu().numpy()
            ref_embeddings_normalized = ref_embeddings_normalized / np.linalg.norm(
                ref_embeddings_normalized, axis=1, keepdims=True
            )
            
            self.reference_index.add(ref_embeddings_normalized.astype('float32'))
            print(f"âœ… FAISSç´¢å¼•åˆ›å»ºå®Œæˆï¼Œå‚è€ƒæ•°æ®é‡: {self.reference_index.ntotal}")
        except Exception as e:
            print(f"âš ï¸ FAISSç´¢å¼•åˆ›å»ºå¤±è´¥: {e}")
            self.reference_index = None
    
    def compute_cosine_distance_matrix(self, embeddings):
        """è®¡ç®—ä½™å¼¦è·ç¦»çŸ©é˜µ"""
        try:
            # ç¡®ä¿è½¬æ¢ä¸ºnumpyæ•°ç»„
            if isinstance(embeddings, torch.Tensor):
                embeddings_np = embeddings.detach().cpu().numpy()
            else:
                embeddings_np = np.array(embeddings)
            
            # ç¡®ä¿æ˜¯float64ç±»å‹ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§
            embeddings_np = embeddings_np.astype(np.float64)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
            cosine_sim_matrix = cosine_similarity(embeddings_np)
            # è½¬æ¢ä¸ºè·ç¦»çŸ©é˜µ
            distance_matrix = 1 - cosine_sim_matrix
            
            # ç¡®ä¿è¿”å›çš„æ˜¯numpyæ•°ç»„
            return np.array(distance_matrix, dtype=np.float64)
            
        except Exception as e:
            print(f"âš ï¸ ä½™å¼¦è·ç¦»çŸ©é˜µè®¡ç®—å¤±è´¥: {e}")
            n_samples = len(embeddings) if hasattr(embeddings, '__len__') else 1
            return np.ones((n_samples, n_samples), dtype=np.float64)
    
    def compute_local_density(self, embeddings, n_neighbors=10, power=0.5):
        """è®¡ç®—å±€éƒ¨å¯†åº¦"""
        try:
            distance_matrix = self.compute_cosine_distance_matrix(embeddings)
            n_samples = distance_matrix.shape[0]
            n_neighbors = min(n_neighbors, n_samples - 1)
            
            if n_neighbors <= 0:
                return np.ones(n_samples, dtype=np.float64)
            
            densities = []
            for i in range(n_samples):
                # è·å–åˆ°å…¶ä»–æ‰€æœ‰ç‚¹çš„è·ç¦»ï¼ˆæ’é™¤è‡ªå·±ï¼‰
                distances = distance_matrix[i].copy()
                distances = np.delete(distances, i)
                
                if len(distances) == 0:
                    densities.append(1.0)
                    continue
                
                # æ‰¾åˆ°kä¸ªæœ€è¿‘é‚»
                if len(distances) >= n_neighbors:
                    nearest_distances = np.partition(distances, n_neighbors-1)[:n_neighbors]
                else:
                    nearest_distances = distances
                
                # è®¡ç®—å±€éƒ¨å¯†åº¦ï¼ˆè·ç¦»è¶Šå°ï¼Œå¯†åº¦è¶Šé«˜ï¼‰
                avg_distance = np.mean(nearest_distances)
                density = 1.0 / (1.0 + avg_distance)  # è½¬æ¢ä¸ºå¯†åº¦å€¼
                density = density ** power  # åº”ç”¨å¹‚æ¬¡è°ƒæ•´
                densities.append(float(density))
            
            return np.array(densities, dtype=np.float64)
            
        except Exception as e:
            print(f"âš ï¸ å±€éƒ¨å¯†åº¦è®¡ç®—å¤±è´¥: {e}")
            n_samples = len(embeddings) if hasattr(embeddings, '__len__') else 1
            return np.ones(n_samples, dtype=np.float64)
    
    def weighted_average(self, row, power=1.0):
        """è®¡ç®—å¹³å‡è·ç¦» - ä¿®å¤ç‰ˆæœ¬"""
        try:
            # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„
            row = np.array(row, dtype=np.float64)
            
            # æ’é™¤è‡ªå·±ï¼ˆå¯¹è§’çº¿å…ƒç´ åº”è¯¥æ˜¯0ï¼‰
            non_zero_distances = row[row > 1e-10]
            
            if len(non_zero_distances) > 0:
                result = np.mean(non_zero_distances)
            else:
                result = np.mean(row)
                
            return float(result)
            
        except Exception as e:
            print(f"âš ï¸ å¹³å‡è·ç¦»è®¡ç®—å¤±è´¥: {e}")
            return float(np.mean(np.array(row))) if hasattr(row, '__len__') else 0.5
    
    def calculate_novelsum_score(self, texts, density_power=0.5, distance_power=1.0, neighbors=10):
        """è®¡ç®—NovelSumå¤šæ ·æ€§åˆ†æ•°"""
        try:
            # åŸºæœ¬éªŒè¯
            if not texts or len(texts) < 2:
                print(f"âš ï¸ æ–‡æœ¬æ•°é‡ä¸è¶³: {len(texts) if texts else 0}")
                return 0.5  # å•ä¸ªæ ·æœ¬è¿”å›ä¸­ç­‰åˆ†æ•°
            
            # è¿‡æ»¤ç©ºæ–‡æœ¬å¹¶ç¡®ä¿æ‰€æœ‰å…ƒç´ éƒ½æ˜¯å­—ç¬¦ä¸²
            valid_texts = []
            for text in texts:
                if text is not None:
                    # ç¡®ä¿è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    str_text = str(text).strip() if not isinstance(text, str) else text.strip()
                    if len(str_text) > 5:
                        valid_texts.append(str_text)
            
            if len(valid_texts) < 2:
                print(f"âš ï¸ æœ‰æ•ˆæ–‡æœ¬æ•°é‡ä¸è¶³: {len(valid_texts)}")
                return 0.5
            
            print(f"ğŸ” DEBUG: è®¡ç®—{len(valid_texts)}ä¸ªæ–‡æœ¬çš„NovelSumåˆ†æ•°")
            
            # ç”Ÿæˆæ–‡æœ¬åµŒå…¥
            try:
                embeddings = self.embedding_model.encode(
                    valid_texts, 
                    convert_to_tensor=True,
                    device=self.device
                )
                print(f"ğŸ” DEBUG: Embeddings shape: {embeddings.shape}")
            except Exception as e:
                print(f"âš ï¸ åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
                print(f"ğŸ” DEBUG: valid_textsç±»å‹æ£€æŸ¥: {[type(t) for t in valid_texts[:3]]}")
                return 0.5
            
            # ç¡®ä¿embeddingsæ˜¯æ­£ç¡®çš„tensorç±»å‹
            if not isinstance(embeddings, torch.Tensor):
                embeddings = torch.tensor(embeddings, device=self.device, dtype=torch.float32)
            
            # æ£€æŸ¥embeddingsçš„å½¢çŠ¶
            if embeddings.dim() != 2 or embeddings.shape[0] != len(valid_texts):
                print(f"âš ï¸ Embeddingså½¢çŠ¶å¼‚å¸¸: {embeddings.shape}, æœŸæœ›: ({len(valid_texts)}, embedding_dim)")
                return 0.5
            
            # è®¡ç®—å±€éƒ¨å¯†åº¦
            try:
                densities = self.compute_local_density(embeddings, neighbors, density_power)
                print(f"ğŸ” DEBUG: Densities shape: {densities.shape}, æ ·ä¾‹å€¼: {densities[:3]}")
            except Exception as e:
                print(f"âš ï¸ å±€éƒ¨å¯†åº¦è®¡ç®—å¼‚å¸¸: {e}")
                return 0.5
            
            # è®¡ç®—è·ç¦»çŸ©é˜µ
            try:
                distance_matrix = self.compute_cosine_distance_matrix(embeddings)
                print(f"ğŸ” DEBUG: Distance matrix shape: {distance_matrix.shape}")
            except Exception as e:
                print(f"âš ï¸ è·ç¦»çŸ©é˜µè®¡ç®—å¼‚å¸¸: {e}")
                return 0.5
            
            # è®¡ç®—NovelSumåˆ†æ•° - ä¿®å¤ç‰ˆæœ¬
            scores = []
            for i in range(len(valid_texts)):
                try:
                    row = distance_matrix[i]
                    density_weight = float(densities[i])
                    
                    # è®¡ç®—å¹³å‡è·ç¦»ï¼ˆå¤šæ ·æ€§åº¦é‡ï¼‰
                    avg_distance = self.weighted_average(row, distance_power)
                    
                    # ç»“åˆå¯†åº¦è¿›è¡Œè°ƒæ•´
                    # è·ç¦»è¶Šå¤§è¡¨ç¤ºè¶Šæ–°é¢–ï¼Œå¯†åº¦è¶Šé«˜è¡¨ç¤ºè¯¥åŒºåŸŸè¶Šæ‹¥æŒ¤
                    # ä½¿ç”¨å¯†åº¦çš„å€’æ•°ä½œä¸ºè°ƒæ•´å› å­ï¼Œé¿å…æ•°å€¼è¿‡å¤§
                    density_factor = 1.0 / (density_weight + 1e-8)
                    novelsum_score = avg_distance * (1.0 + density_factor)
                    
                    scores.append(float(novelsum_score))
                    
                except Exception as e:
                    print(f"âš ï¸ ç¬¬{i}ä¸ªæ ·æœ¬åˆ†æ•°è®¡ç®—å¤±è´¥: {e}")
                    scores.append(0.5)
            
            # è¿”å›å¹³å‡åˆ†æ•°
            if not scores:
                print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆåˆ†æ•°")
                return 0.5
                
            final_score = float(np.mean(scores))
            print(f"ğŸ” DEBUG: æœ€ç»ˆNovelSumåˆ†æ•°: {final_score}")
            
            # æ£€æŸ¥åˆ†æ•°çš„åˆç†æ€§
            if final_score < 0 or final_score > 10 or np.isnan(final_score) or np.isinf(final_score):
                print(f"âš ï¸ åˆ†æ•°å¼‚å¸¸: {final_score}, ä½¿ç”¨é»˜è®¤å€¼")
                return 0.5
                
            return final_score
            
        except Exception as e:
            print(f"âš ï¸ NovelSumåˆ†æ•°è®¡ç®—å¤±è´¥: {e}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return 0.5
    
    def calculate_internal_diversity(self, texts):
        """è®¡ç®—å†…éƒ¨å¤šæ ·æ€§ï¼ˆæ‰¹æ¬¡å†…æ ·æœ¬é—´çš„å¹³å‡è·ç¦»ï¼‰"""
        try:
            if len(texts) < 2:
                return 0.5
            
            embeddings = self.embedding_model.encode(
                texts, 
                convert_to_tensor=True,
                device=self.device
            )
            
            distance_matrix = self.compute_cosine_distance_matrix(embeddings)
            
            # è®¡ç®—ä¸Šä¸‰è§’çŸ©é˜µçš„å¹³å‡å€¼ï¼ˆæ’é™¤å¯¹è§’çº¿ï¼‰
            n = distance_matrix.shape[0]
            total_pairs = n * (n - 1) // 2
            
            if total_pairs == 0:
                return 0.5
            
            diversity_sum = 0
            for i in range(n):
                for j in range(i + 1, n):
                    diversity_sum += distance_matrix[i, j]
            
            average_diversity = diversity_sum / total_pairs
            return float(average_diversity)
            
        except Exception as e:
            print(f"âš ï¸ å†…éƒ¨å¤šæ ·æ€§è®¡ç®—å¤±è´¥: {e}")
            return 0.5

def calculate_restaurant_specific_diversity(texts):
    """è®¡ç®—é¤å…è¯„è®ºç‰¹è‰²å¤šæ ·æ€§"""
    try:
        if len(texts) < 2:
            return 0.5
        
        # é¤å…è¯„è®ºç‰¹å®šçš„å¤šæ ·æ€§æŒ‡æ ‡
        aspect_keywords = {
            'food_quality': ['delicious', 'tasty', 'bland', 'awful', 'amazing', 'terrible', 'fresh', 'stale'],
            'service': ['friendly', 'rude', 'quick', 'slow', 'attentive', 'negligent', 'professional'],
            'atmosphere': ['cozy', 'loud', 'romantic', 'casual', 'elegant', 'crowded', 'peaceful'],
            'price': ['expensive', 'cheap', 'reasonable', 'overpriced', 'value', 'affordable', 'costly'],
            'location': ['convenient', 'remote', 'accessible', 'parking', 'downtown', 'suburban']
        }
        
        # åˆ†ææ¯ä¸ªæ–‡æœ¬è¦†ç›–çš„æ–¹é¢
        text_aspects = []
        for text in texts:
            text_lower = text.lower()
            aspects = set()
            
            for aspect, keywords in aspect_keywords.items():
                if any(keyword in text_lower for keyword in keywords):
                    aspects.add(aspect)
            
            text_aspects.append(aspects)
        
        # è®¡ç®—æ–¹é¢è¦†ç›–å¤šæ ·æ€§
        all_aspects = set()
        for aspects in text_aspects:
            all_aspects.update(aspects)
        
        if not all_aspects:
            return 0.3  # æ²¡æœ‰è¯†åˆ«åˆ°ç‰¹å®šæ–¹é¢
        
        # è®¡ç®—Jaccardå¤šæ ·æ€§
        diversities = []
        for i in range(len(text_aspects)):
            for j in range(i + 1, len(text_aspects)):
                aspects1, aspects2 = text_aspects[i], text_aspects[j]
                if len(aspects1) == 0 and len(aspects2) == 0:
                    jaccard = 1.0  # éƒ½ä¸ºç©ºï¼Œå®Œå…¨ç›¸ä¼¼
                else:
                    intersection = len(aspects1.intersection(aspects2))
                    union = len(aspects1.union(aspects2))
                    jaccard = intersection / union if union > 0 else 0.0
                
                diversity = 1.0 - jaccard  # è½¬æ¢ä¸ºå¤šæ ·æ€§
                diversities.append(diversity)
        
        return np.mean(diversities) if diversities else 0.5
        
    except Exception as e:
        print(f"âš ï¸ é¤å…ç‰¹è‰²å¤šæ ·æ€§è®¡ç®—å¤±è´¥: {e}")
        return 0.5

def extract_text_content_global(completion):
    """å…¨å±€å‡½æ•°ï¼šä»completionä¸­æå–å®é™…çš„æ–‡æœ¬å†…å®¹ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
    try:
        # é¦–å…ˆåˆ†ç¦»å‡ºç”Ÿæˆå†…å®¹
        generation = separate_prompt_and_generation_global(completion)
        
        # ä»ç”Ÿæˆå†…å®¹ä¸­æå–æ–‡æœ¬
        if "Text:" in generation:
            text_part = generation.split("Text:")[1]
            if "Label:" in text_part:
                text_part = text_part.split("Label:")[0]
            return text_part.strip()
        
        # å°è¯•ä»JSONä¸­æå–
        if "{" in generation and "}" in generation:
            try:
                import json
                start_idx = generation.find("{")
                end_idx = generation.rfind("}") + 1
                json_str = generation[start_idx:end_idx]
                parsed_data = json.loads(json_str)
                input_text = parsed_data.get("input", "")
                if input_text.startswith("Text: "):
                    return input_text[6:]
                elif input_text:
                    return input_text
            except json.JSONDecodeError:
                pass
        
        return generation.strip()[:200]  # å¦‚æœæ²¡æœ‰æ ¼å¼ï¼Œè¿”å›å‰200å­—ç¬¦
    except:
        return completion.strip()[:200]

def separate_prompt_and_generation_global(completion):
    """å…¨å±€å‡½æ•°ï¼šä»GRPOçš„completionä¸­åˆ†ç¦»å‡ºçœŸæ­£çš„æ¨¡å‹ç”Ÿæˆå†…å®¹"""
    try:
        # æ–¹æ³•1: æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
        json_start = completion.find('{')
        if json_start != -1:
            # ä»ç¬¬ä¸€ä¸ª{å¼€å§‹ï¼Œæ‰¾åˆ°åŒ¹é…çš„}
            brace_count = 0
            json_end = -1
            
            for i in range(json_start, len(completion)):
                if completion[i] == '{':
                    brace_count += 1
                elif completion[i] == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        json_end = i
                        break
            
            if json_end != -1:
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSON
                first_json = completion[json_start:json_end + 1]
                
                # éªŒè¯è¿™æ˜¯å¦æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„JSON
                try:
                    import json
                    json.loads(first_json)
                    return first_json  # è¿”å›ç¬¬ä¸€ä¸ªæœ‰æ•ˆJSON
                except json.JSONDecodeError:
                    pass  # å¦‚æœä¸æ˜¯æœ‰æ•ˆJSONï¼Œç»§ç»­å…¶ä»–æ–¹æ³•
        
        # æ–¹æ³•2: æŸ¥æ‰¾assistantæ ‡è®°
        assistant_markers = [
            'assistant:',
            'Assistant:',
            'åŠ©æ‰‹:',
        ]
        
        for marker in assistant_markers:
            pos = completion.find(marker)
            if pos != -1:
                remaining = completion[pos + len(marker):].strip()
                if len(remaining) > 30:  # ç¡®ä¿æœ‰è¶³å¤Ÿå†…å®¹
                    return remaining
        
        # æ–¹æ³•3: æŸ¥æ‰¾ç”Ÿæˆå†…å®¹çš„å¼€å§‹æ ‡è®°
        generation_markers = [
            'Here is',
            'here is',
            'Based on',
            'based on',
            'æ ¹æ®',
            'æŒ‰ç…§',
            'ä»¥ä¸‹æ˜¯',
        ]
        
        for marker in generation_markers:
            pos = completion.find(marker)
            if pos != -1 and pos > len(completion) * 0.2:  # åœ¨å80%ä½ç½®
                remaining = completion[pos:].strip()
                if len(remaining) > 50:  # ç¡®ä¿æœ‰è¶³å¤Ÿå†…å®¹
                    return remaining
        
        # æ–¹æ³•4: å¦‚æœcompletionè¾ƒé•¿ï¼Œå¯èƒ½å‰é¢æ˜¯prompté‡å¤ï¼Œå–ååŠéƒ¨åˆ†
        if len(completion) > 1000:  # å¯¹äºé•¿æ–‡æœ¬
            split_point = int(len(completion) * 0.6)
            return completion[split_point:].strip()
        
        # æœ€åçš„fallback
        return completion.strip()
        
    except Exception as e:
        print(f"âš ï¸ åˆ†ç¦»promptå’Œgenerationæ—¶å‡ºé”™: {e}")
        return completion.strip()